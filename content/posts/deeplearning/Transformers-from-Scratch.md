+++
title = 'Transformers From Scratch'
date = 2025-08-05T21:34:44+09:00
draft = false
+++

원문: https://e2eml.school/transformers.html

트랜스포머의 깊은 원리를 이해하기 위해 오랜 고민 끝에 마침내 그 탐구에 착수했습니다. 이 글은 그 탐구의 결과물입니다.

트랜스포머는 2017년 [논문](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf "null")에서 '시퀀스 변환(sequence transduction)' 도구로 처음 소개되었습니다. 시퀀스 변환이란 한 종류의 기호 시퀀스를 다른 종류의 기호 시퀀스로 변환하는 것을 의미합니다. 가장 대표적인 예시는 영어-독일어 번역과 같은 언어 번역입니다. 또한, 주어진 시작 프롬프트에 이어지는 내용을 같은 맥락과 스타일로 생성하는 '시퀀스 완성(sequence completion)' 작업에도 활용되도록 발전했습니다. 트랜스포머는 빠르게 자연어 처리(NLP) 분야의 연구와 제품 개발에 없어서는 안 될 필수적인 도구가 되었습니다.

시작하기 전에 미리 말씀드립니다. 이 글에서는 행렬 곱셈과 역전파(모델 학습 알고리즘)에 대해 많이 다룰 예정이지만, 미리 알고 있을 필요는 없습니다. 필요한 개념들은 하나씩 설명과 함께 추가할 것입니다.

짧지 않은 여정이 되겠지만, 여러분이 이 여정에 함께해 주신 것을 기쁘게 생각합니다.

### 원-핫 인코딩 (One-hot Encoding)

모든 것의 시작은 단어였습니다. 방대한 양의 단어들이 존재했죠. 우리의 첫 번째 단계는 모든 단어를 숫자로 변환하여 수학적 연산을 할 수 있도록 하는 것입니다.

우리의 목표가 음성 명령에 응답하는 컴퓨터를 만드는 것이라고 상상해 봅시다. 음성 시퀀스를 단어 시퀀스로 변환(또는 **변형(transduce)**)하는 트랜스포머를 구축하는 것이 우리의 임무입니다.

우리는 각 시퀀스에서 작업할 기호들의 모음인 **어휘(vocabulary)** 를 선택하는 것부터 시작합니다. 이 경우, 입력 시퀀스에는 음성 소리를 나타내는 기호 집합이, 출력 시퀀스에는 단어를 나타내는 기호 집합이 두 가지 다르게 존재할 것입니다.

지금은 영어를 사용한다고 가정해 봅시다. 영어에는 수만 개의 단어가 있고, 컴퓨터 관련 용어를 포함하면 수천 개가 더 추가될 수 있습니다. 이렇게 되면 어휘 크기는 거의 십만 개에 달할 것입니다. 단어를 숫자로 변환하는 한 가지 방법은 1부터 세기 시작하여 각 단어에 고유한 숫자를 할당하는 것입니다. 그러면 단어 시퀀스는 숫자 목록으로 표현될 수 있습니다.

예를 들어, 'files', 'find', 'my' 세 단어로 이루어진 작은 언어를 생각해 봅시다. 각 단어는 숫자로 대체될 수 있습니다. 예를 들어 'files' = 1, 'find' = 2, 'my' = 3과 같이요. 그러면 "Find my files"라는 문장은 단어 시퀀스 [ _find_, _my_, _files_ ]로 구성되며, 대신 숫자 시퀀스 [2, 3, 1]로 표현될 수 있습니다.

이것은 기호를 숫자로 변환하는 완벽하게 유효한 방법이지만, 컴퓨터가 작업하기에 훨씬 더 쉬운 다른 형식인 **원-핫 인코딩(one-hot encoding)** 이 있습니다. 원-핫 인코딩에서는 기호가 대부분 0으로 이루어진 배열로 표현되며, 이 배열은 어휘의 길이와 같고 단 하나의 요소만 1의 값을 가집니다. 배열의 각 요소는 별도의 기호에 해당합니다.

원-핫 인코딩을 생각하는 또 다른 방법은 각 단어에 여전히 고유한 숫자가 할당되지만, 이제 그 숫자가 배열의 인덱스라는 것입니다. 다음은 원-핫 표기법으로 표현된 위 예시입니다.

![](/posts/Transformers-from-Scratch/one_hot_vocabulary.png)

따라서 "Find my files"라는 문장은 1차원 배열의 시퀀스가 되며, 이들을 함께 배치하면 2차원 배열의 형태를 띠게 됩니다.

![](/posts/Transformers-from-Scratch/one_hot_sentence.png)

참고로, 저는 "1차원 배열"과 "**벡터(vector)**"를 같은 의미로 사용할 것입니다. 마찬가지로 "2차원 배열"과 "**행렬(matrix)**"도 같은 의미로 사용합니다.

### 내적 (Dot product)

원-핫 표현의 한 가지 정말 유용한 점은 [내적(dot product)](https://en.wikipedia.org/wiki/Dot_product "null")을 계산할 수 있다는 것입니다. 이는 내적(inner product) 및 스칼라 곱(scalar product)과 같은 다른 용어로도 알려져 있습니다. 두 벡터의 내적을 구하려면 해당 요소들을 곱한 다음, 그 결과들을 더합니다.

![](/posts/Transformers-from-Scratch/dot_product.png)

내적은 원-핫 단어 표현과 함께 작업할 때 특히 유용합니다. 어떤 원-핫 벡터와 자기 자신의 내적은 1입니다.

![](/posts/Transformers-from-Scratch/match.png)

그리고 어떤 원-핫 벡터와 다른 원-핫 벡터의 내적은 0입니다.

![](/posts/Transformers-from-Scratch/non_match.png)

앞의 두 예시는 내적이 유사성을 측정하는 데 어떻게 사용될 수 있는지 보여줍니다. 다른 예시로, 다양한 가중치를 가진 단어들의 조합을 나타내는 값들의 벡터를 생각해 봅시다. 원-핫 인코딩된 단어는 내적을 통해 해당 단어가 얼마나 강하게 표현되는지 보여주기 위해 비교될 수 있습니다.

![](/posts/Transformers-from-Scratch/similarity.png)

### 행렬 곱셈 (Matrix multiplication)

내적은 행렬 곱셈의 빌딩 블록이며, 이는 두 개의 2차원 배열을 결합하는 매우 특정한 방법입니다. 이 행렬들 중 첫 번째를 A라고 하고 두 번째를 B라고 부르겠습니다. 가장 간단한 경우, A가 하나의 행만 가지고 있고 B가 하나의 열만 가지고 있을 때, 행렬 곱셈의 결과는 두 벡터의 내적입니다.

![](/posts/Transformers-from-Scratch/matrix_mult_one_row_one_col.png)

A의 열 수와 B의 행 수가 같아야 두 배열이 일치하고 내적이 작동한다는 점에 유의하십시오.

A와 B가 커지기 시작하면 행렬 곱셈은 복잡해지기 시작합니다. A에 여러 행이 있는 경우를 처리하려면, B와 각 행을 개별적으로 내적합니다. 결과는 A가 가진 행 수만큼의 행을 가질 것입니다.

![](/posts/Transformers-from-Scratch/matrix_mult_two_row_one_col.png)

B가 더 많은 열을 가질 때, 각 열을 A와 내적하고 결과를 연속적인 열로 쌓습니다.

![](/posts/Transformers-from-Scratch/matrix_mult_one_row_two_col.png)

이제 A의 열 수와 B의 행 수가 같은 한, 어떤 두 행렬도 곱할 수 있습니다. 결과는 A와 같은 수의 행을 가지고 B와 같은 수의 열을 가질 것입니다.

![](/posts/Transformers-from-Scratch/matrix_mult_three_row_two_col.png)

이것을 처음 본다면 불필요하게 복잡하게 느껴질 수도 있지만, 나중에 큰 도움이 될 것이라고 약속합니다.

#### 테이블 조회로서의 행렬 곱셈 (Matrix multiplication as a table lookup)

여기서 행렬 곱셈이 어떻게 조회 테이블 역할을 하는지 주목하십시오. 우리의 A 행렬은 원-핫 벡터들의 스택으로 구성됩니다. 이들은 각각 첫 번째 열, 네 번째 열, 세 번째 열에 1을 가지고 있습니다. 행렬 곱셈을 수행할 때, 이것은 B 행렬의 첫 번째 행, 네 번째 행, 세 번째 행을 순서대로 추출하는 역할을 합니다. 행렬의 특정 행을 추출하기 위해 원-핫 벡터를 사용하는 이 트릭은 트랜스포머가 작동하는 방식의 핵심입니다.

### 1차 시퀀스 모델 (First order sequence model)

잠시 행렬을 제쳐두고 우리가 정말 관심 있는 것, 즉 단어 시퀀스로 돌아가 봅시다. 자연어 컴퓨터 인터페이스를 개발하기 시작할 때 다음 세 가지 가능한 명령만 처리하고 싶다고 상상해 봅시다.

- _Show me my directories please_.
- _Show me my files please_.
- _Show me my photos please_.

이제 우리의 어휘 크기는 7개입니다: { _directories, files, me, my, photos, please, show_ }.

시퀀스를 표현하는 한 가지 유용한 방법은 전이 모델(transition model)을 사용하는 것입니다. 어휘의 모든 단어에 대해 다음 단어가 무엇일 가능성이 높은지 보여줍니다. 사용자가 절반은 사진에 대해 묻고, 30%는 파일에 대해 묻고, 나머지는 디렉토리에 대해 묻는다면 전이 모델은 다음과 같이 보일 것입니다. 어떤 단어에서 벗어나는 전이의 합은 항상 1이 됩니다.

![](/posts/Transformers-from-Scratch/markov_chain.png)

이 특정 전이 모델은 **마르코프 연쇄(Markov chain)** 라고 불리는데, 이는 다음 단어에 대한 확률이 최근 단어에만 의존한다는 [마르코프 속성(Markov property)](https://en.wikipedia.org/wiki/Markov_property "null")을 만족하기 때문입니다. 더 구체적으로 말하면, 단 하나의 가장 최근 단어만 보기 때문에 1차 마르코프 모델입니다. 가장 최근 두 단어를 고려한다면 2차 마르코프 모델이 될 것입니다.

행렬과의 휴식은 끝났습니다. 마르코프 연쇄는 행렬 형태로 편리하게 표현될 수 있습니다. 원-핫 벡터를 생성할 때 사용했던 것과 동일한 인덱싱 체계를 사용하여 각 행은 어휘의 단어 중 하나를 나타냅니다. 각 열도 마찬가지입니다. 행렬 전이 모델은 행렬을 조회 테이블로 취급합니다. 관심 있는 단어에 해당하는 행을 찾으십시오. 각 열의 값은 다음으로 올 단어의 확률을 보여줍니다. 행렬의 각 요소의 값이 확률을 나타내므로 모두 0과 1 사이에 있을 것입니다. 확률은 항상 1로 합산되므로 각 행의 값은 항상 1이 됩니다.

![](/posts/Transformers-from-Scratch/transition_matrix.png)

여기 전이 행렬에서 우리는 세 문장의 구조를 명확하게 볼 수 있습니다. 거의 모든 전이 확률은 0 또는 1입니다. 마르코프 연쇄에서 분기가 발생하는 곳은 단 한 곳뿐입니다. _my_ 다음에는 _directories_, _files_, _photos_ 단어가 각각 다른 확률로 나타날 수 있습니다. 그 외에는 다음 단어가 무엇일지에 대한 불확실성이 없습니다. 이러한 확실성은 전이 행렬에 대부분 1과 0이 있는 것으로 반영됩니다.

원-핫 벡터를 사용하여 행렬 곱셈을 통해 주어진 단어와 관련된 전이 확률을 추출하는 트릭을 다시 사용할 수 있습니다. 예를 들어, _my_ 다음에 어떤 단어가 올 확률만 분리하고 싶다면, _my_ 단어를 나타내는 원-핫 벡터를 생성하고 이를 전이 행렬과 곱하면 됩니다. 이렇게 하면 관련 행이 추출되고 다음 단어가 무엇일지에 대한 확률 분포를 보여줍니다.

![](/posts/Transformers-from-Scratch/transition_lookups.png)

### 2차 시퀀스 모델 (Second order sequence model)

현재 단어만을 기반으로 다음 단어를 예측하는 것은 어렵습니다. 그것은 첫 음표만 주어졌을 때 곡의 나머지를 예측하는 것과 같습니다. 적어도 두 음표를 얻을 수 있다면 우리의 기회는 훨씬 더 좋습니다.

컴퓨터 명령을 위한 또 다른 장난감 언어 모델에서 이것이 어떻게 작동하는지 볼 수 있습니다. 이 모델은 40/60 비율로 두 문장만 볼 것으로 예상됩니다.

- _Check whether the battery ran down please._
- _Check whether the program ran please._

마르코프 연쇄는 이에 대한 1차 모델을 보여줍니다.

![](/posts/Transformers-from-Scratch/markov_chain_2.png)

여기서 우리는 모델이 단어 하나만 보는 대신 가장 최근 두 단어를 본다면 더 나은 작업을 수행할 수 있다는 것을 알 수 있습니다. _battery ran_ 을 만나면 다음 단어가 _down_ 이라는 것을 알고, _program ran_ 을 보면 다음 단어가 _please_ 라는 것을 압니다. 이것은 모델의 분기 중 하나를 제거하여 불확실성을 줄이고 신뢰도를 높입니다. 두 단어를 뒤로 보는 것은 이것을 2차 마르코프 모델로 바꿉니다. 이것은 다음 단어 예측의 기반이 될 더 많은 문맥을 제공합니다. 2차 마르코프 연쇄는 그리기 더 어렵지만, 다음은 그 가치를 보여주는 연결입니다.

![](/posts/Transformers-from-Scratch/markov_chain_second_order.png)

둘 사이의 차이를 강조하기 위해 다음은 1차 전이 행렬입니다.

![](/posts/Transformers-from-Scratch/transition_matrix_first_order_2.png)

그리고 다음은 2차 전이 행렬입니다.

![](/posts/Transformers-from-Scratch/transition_matrix_second_order.png)

2차 행렬에는 모든 단어 조합에 대한 별도의 행이 있다는 점에 유의하십시오(대부분은 여기에 표시되지 않음). 즉, 어휘 크기가 N으로 시작하면 전이 행렬에는 $N^2$개의 행이 있습니다.

이것이 우리에게 주는 것은 더 많은 신뢰입니다. 2차 모델에는 1이 더 많고 분수가 더 적습니다. 분수가 있는 행은 하나뿐이며, 우리 모델에는 하나의 분기만 있습니다. 직관적으로, 단어 하나만 보는 대신 두 단어를 보는 것은 다음 단어를 추측하는 데 더 많은 문맥, 더 많은 정보를 제공합니다.

### 건너뛰기를 포함한 2차 시퀀스 모델 (Second order sequence model with skips)

2차 모델은 다음 단어를 결정하기 위해 두 단어만 뒤로 돌아보면 될 때 잘 작동합니다. 더 멀리 뒤로 돌아봐야 할 때는 어떨까요? 또 다른 언어 모델을 구축하고 있다고 상상해 봅시다. 이 모델은 두 문장만 나타내야 하며, 각각 발생할 확률은 같습니다.

- _Check the program log and find out whether it ran please._
- _Check the battery log and find out whether it ran down please._

이 예시에서 _ran_ 다음에 어떤 단어가 와야 하는지 결정하려면 8단어 뒤로 돌아봐야 합니다. 8차 모델을 단순히 구현할 경우 $N^8$개의 행을 가지게 되며, 이는 일반적인 어휘 크기에서는 비현실적인 수치입니다.

대신, 우리는 교활한 일을 할 수 있습니다. 2차 모델을 만들지만, 가장 최근 단어와 그 이전에 온 각 단어의 조합을 고려하는 것입니다. 여전히 2차 모델입니다. 한 번에 두 단어만 고려하기 때문입니다. 하지만 이것은 더 멀리까지 도달하여 **장거리 의존성(long range dependencies)** 을 포착할 수 있도록 합니다. 이 건너뛰기를 포함한 2차 모델과 완전한 n차 모델의 차이점은 우리가 단어 순서 정보와 선행 단어의 조합 대부분을 버린다는 것입니다. 남아 있는 것은 여전히 매우 강력합니다.

마르코프 연쇄는 이제 우리를 완전히 실패하게 하지만, 우리는 여전히 각 선행 단어 쌍과 뒤따르는 단어 사이의 연결을 나타낼 수 있습니다. 여기서는 숫자 가중치를 없애고, 대신 0이 아닌 가중치와 관련된 화살표만 보여주고 있습니다. 더 큰 가중치는 더 굵은 선으로 표시됩니다.

![](/posts/Transformers-from-Scratch/feature_voting.png)

다음은 전이 행렬에서 어떻게 보이는지입니다.

![](/posts/Transformers-from-Scratch/transition_matrix_second_order_skips.png)

이 보기는 _ran_ 다음에 오는 단어를 예측하는 데 관련된 행만 보여줍니다. 가장 최근 단어(_ran_)가 어휘의 다른 각 단어에 의해 선행되는 경우를 보여줍니다. 관련 값만 표시됩니다. 모든 빈 셀은 0입니다.

가장 먼저 분명해지는 것은 _ran_ 다음에 오는 단어를 예측하려고 할 때, 더 이상 한 줄만 보는 것이 아니라 전체 집합을 본다는 것입니다. 우리는 이제 마르코프 영역을 벗어났습니다. 각 행은 더 이상 특정 지점의 시퀀스 상태를 나타내지 않습니다. 대신, 각 행은 특정 지점에서 시퀀스를 설명할 수 있는 여러 **특징(features)** 중 하나를 나타냅니다. 가장 최근 단어와 그 이전에 온 각 단어의 조합은 적용 가능한 행들의 모음, 아마도 큰 모음을 만듭니다. 이러한 의미의 변화로 인해 행렬의 각 값은 더 이상 확률을 나타내지 않고, 오히려 투표를 나타냅니다. 투표는 합산되고 비교되어 다음 단어 예측을 결정합니다.

다음으로 분명해지는 것은 대부분의 특징이 중요하지 않다는 것입니다. 대부분의 단어는 두 문장에 모두 나타나므로, 이들이 나타났다는 사실은 다음에 올 단어를 예측하는 데 아무런 도움이 되지 않습니다. 이들은 모두 .5의 값을 가집니다. 유일한 두 가지 예외는 _battery_ 와 _program_ 입니다. 이들은 우리가 구별하려는 두 가지 경우와 관련된 1과 0의 가중치를 가집니다. 특징 _battery, ran_ 은 _ran_ 이 가장 최근 단어이고 _battery_ 가 문장의 어딘가에 이전에 나타났음을 나타냅니다. 이 특징은 _down_ 과 관련된 가중치 1을 가지고 _please_ 와 관련된 가중치 0을 가집니다. 유사하게, 특징 _program, ran_ 은 반대되는 가중치 집합을 가집니다. 이 구조는 이 두 단어가 문장 초기에 존재한다는 것이 다음 단어를 예측하는 데 결정적이라는 것을 보여줍니다.

이 단어 쌍 특징 집합을 다음 단어 추정치로 변환하려면, 모든 관련 행의 값을 합산해야 합니다. 열을 따라 더하면, 시퀀스 _Check the program log and find out whether it ran_ 은 _down_ 에 대해 4, _please_ 에 대해 5를 제외하고는 모든 단어에 대해 0의 합계를 생성합니다. 시퀀스 _Check the battery log and find out whether it ran_ 은 동일하게 수행하지만, _down_ 에 대해 5, _please_ 에 대해 4를 가집니다. 가장 높은 투표 합계를 가진 단어를 다음 단어 예측으로 선택함으로써, 이 모델은 8단어 깊은 의존성을 가짐에도 불구하고 올바른 답을 얻습니다.

### 마스킹 (Masking)

더 신중하게 고려하면, 이것은 만족스럽지 않습니다. 투표 총계 4와 5의 차이는 상대적으로 작습니다. 이는 모델이 기대만큼 확신하지 못한다는 것을 시사합니다. 그리고 더 크고 유기적인 언어 모델에서는 이러한 미미한 차이가 통계적 노이즈에 의해 사라질 수 있다고 쉽게 상상할 수 있습니다.

우리는 유익하지 않은 모든 특징 투표를 제거하여 예측을 날카롭게 할 수 있습니다. _battery, ran_ 과 _program, ran_ 을 제외하고요. 이 시점에서 우리가 전이 행렬에서 [관련 행](https://e2eml.school/#table_lookup "null")을 현재 활성 상태인 특징을 보여주는 벡터와 곱하여 추출한다는 것을 기억하는 것이 도움이 됩니다. 이 예시에서는 지금까지 여기에 표시된 암시적 특징 벡터를 사용해 왔습니다.

![](/posts/Transformers-from-Scratch/feature_selection.png)

여기에는 _ran_ 과 그 이전에 오는 각 단어의 조합인 각 특징에 대해 1이 포함됩니다. 그 이후에 오는 단어는 특징 집합에 포함되지 않습니다. (다음 단어 예측 문제에서는 아직 보이지 않았으므로 다음에 올 것을 예측하는 데 사용하는 것은 공정하지 않습니다.) 그리고 이것은 다른 모든 가능한 단어 조합을 포함하지 않습니다. 이 예시에서는 모두 0이 될 것이므로 안전하게 무시할 수 있습니다.

결과를 개선하기 위해 **마스크(mask)** 를 생성하여 도움이 되지 않는 특징들을 추가적으로 0으로 강제할 수 있습니다. 마스크는 숨기거나 마스킹하고 싶은 위치를 제외하고는 모두 1로 채워진 벡터이며, 해당 위치는 0으로 설정됩니다. 우리의 경우, 도움이 되는 유일한 두 특징인 _battery, ran_ 과 _program, ran_ 을 제외한 모든 것을 마스킹하고 싶습니다.

![](/posts/Transformers-from-Scratch/masked_feature_activities.png)

마스크를 적용하려면 두 벡터를 요소별로 곱합니다. 마스킹되지 않은 위치의 특징 활동 값은 1과 곱해져 변경되지 않습니다. 마스킹된 위치의 특징 활동 값은 0과 곱해져 0으로 강제됩니다.

마스크는 전이 행렬의 많은 부분을 숨기는 효과가 있습니다. _ran_ 과 _battery_ 및 _program_ 의 조합을 제외한 모든 것을 숨겨서 중요한 특징만 남깁니다.

![](/posts/Transformers-from-Scratch/masked_transition_matrix.png)

도움이 되지 않는 특징을 마스킹한 후, 다음 단어 예측은 훨씬 더 강력해집니다. _battery_ 단어가 문장 초기에 나타나면 _ran_ 다음 단어는 가중치 1로 _down_ 으로 예측되고 가중치 0으로 _please_ 로 예측됩니다. 25%의 가중치 차이가 무한대의 차이가 되었습니다. 다음 단어가 무엇일지 의심할 여지가 없습니다. _program_ 이 초기에 나타날 때도 _please_ 에 대해 동일한 강력한 예측이 발생합니다.

이 선택적 마스킹 과정은 트랜스포머에 대한 원본 [논문](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf "null")의 제목에 언급된 **어텐션(attention)** 입니다. 지금까지 설명한 내용은 어텐션이 논문에서 구현되는 방식의 근사치일 뿐입니다. 중요한 개념을 포착하지만 세부 사항은 다릅니다. 나중에 그 간극을 좁힐 것입니다.

### 잠시 멈춰서 쉬어가기 (Rest Stop and an Off Ramp)

여기까지 오신 것을 축하드립니다. 원한다면 멈출 수 있습니다. 건너뛰기를 포함한 선택적 2차 모델은 트랜스포머가 무엇을 하는지, 적어도 디코더 측면에서 생각하는 데 유용한 방법입니다. 이는 OpenAI의 [GPT-3](https://en.wikipedia.org/wiki/GPT-3 "null")와 같은 생성형 언어 모델이 무엇을 하는지 대략적으로 포착합니다. 완전한 이야기를 담지는 못하지만, 핵심적인 내용을 나타냅니다.

다음 섹션에서는 이 직관적인 설명과 트랜스포머가 구현되는 방식 사이의 더 많은 간극을 다룹니다. 이는 주로 세 가지 실질적인 고려 사항에 의해 좌우됩니다.

1. **컴퓨터는 특히 행렬 곱셈에 능숙합니다.** 빠른 행렬 곱셈을 위해 특별히 컴퓨터 하드웨어를 구축하는 전체 산업이 있습니다. 행렬 곱셈으로 표현될 수 있는 모든 계산은 놀랍도록 효율적으로 만들 수 있습니다. 그것은 초고속 열차와 같습니다. 짐을 실을 수 있다면, 매우 빠르게 원하는 곳으로 데려다줄 것입니다.
    
2. **각 단계는 미분 가능해야 합니다.** 지금까지 우리는 장난감 예시만 다루었고, 모든 전이 확률과 마스크 값을 직접 선택하는 사치를 누렸습니다. 즉, 모델 **매개변수(parameters)** 를 직접 선택했습니다. 실제로는 이들은 **역전파(backpropagation)** 를 통해 학습되어야 하며, 이는 각 계산 단계가 미분 가능해야 합니다. 이는 매개변수의 작은 변화에 대해 모델 오류 또는 **손실(loss)** 의 해당 변화를 계산할 수 있음을 의미합니다.
    
3. **그래디언트는 부드럽고 잘 조건화되어야 합니다.** 모든 매개변수에 대한 모든 미분값의 조합은 손실 **그래디언트(gradient)** 입니다. 실제로는 역전파가 잘 작동하려면 그래디언트가 부드러워야 합니다. 즉, 어떤 방향으로 작은 단계를 밟을 때 기울기가 너무 빠르게 변하지 않아야 합니다. 또한 그래디언트가 잘 조건화되어 있을 때 훨씬 더 잘 작동합니다. 즉, 한 방향으로 다른 방향보다 급격하게 커지지 않아야 합니다. 손실 함수를 풍경으로 비유한다면, 그랜드 캐년은 조건이 좋지 않은 풍경일 것입니다. 바닥을 따라 이동하든, 옆으로 올라가든, 이동해야 할 기울기가 매우 다를 것입니다. 반대로, 고전적인 Windows 화면 보호기의 완만한 언덕은 조건이 좋은 그래디언트를 가질 것입니다. 신경망을 설계하는 과학이 미분 가능한 빌딩 블록을 만드는 것이라면, 그 예술은 그래디언트가 너무 빠르게 변하지 않고 모든 방향에서 대략 같은 크기를 갖도록 조각들을 쌓는 것입니다.

### 행렬 곱셈으로서의 어텐션 (Attention as matrix multiplication)

특징 가중치는 훈련에서 각 단어 쌍/다음 단어 전이가 얼마나 자주 발생하는지 세어 쉽게 구축할 수 있지만, 어텐션 마스크는 그렇지 않습니다. 지금까지 우리는 마스크 벡터를 갑자기 만들어냈습니다. 트랜스포머가 관련 마스크를 찾는 방법이 중요합니다. 어떤 종류의 조회 테이블을 사용하는 것이 자연스러울 수 있지만, 이제 우리는 모든 것을 행렬 곱셈으로 표현하는 데 집중하고 있습니다. 우리는 모든 단어에 대한 마스크 벡터를 행렬로 쌓고 가장 최근 단어의 원-핫 표현을 사용하여 관련 마스크를 추출하는 위에서 소개한 동일한 [조회](https://e2eml.school/table_lookup "null") 방법을 사용할 수 있습니다.

![](/posts/Transformers-from-Scratch/mask_matrix_lookup.png)

마스크 벡터 모음을 보여주는 행렬에서는 명확성을 위해 우리가 추출하려는 것만 표시했습니다.

우리는 마침내 논문과 연결하기 시작했습니다. 이 마스크 조회는 어텐션 방정식의 $QK^T$ 항으로 표현됩니다.

![](/posts/Transformers-from-Scratch/attention_equation_QKT.png)

쿼리 Q는 관심 있는 특징을 나타내고, 행렬 K는 마스크 모음을 나타냅니다. 마스크가 행이 아닌 열에 저장되어 있기 때문에 곱하기 전에 전치(T 연산자 사용)해야 합니다. 모든 작업을 마칠 때쯤에는 이에 대한 몇 가지 중요한 수정 사항을 만들겠지만, 이 수준에서는 트랜스포머가 활용하는 미분 가능한 조회 테이블의 개념을 포착합니다.

### 행렬 곱셈으로서의 2차 시퀀스 모델 (Second order sequence model as matrix multiplications)

지금까지 우리가 대충 설명했던 또 다른 단계는 전이 행렬의 구성입니다. 우리는 논리에 대해서는 명확했지만, 행렬 곱셈으로 어떻게 하는지에 대해서는 명확하지 않았습니다.

어텐션 단계의 결과, 즉 가장 최근 단어와 그 이전에 온 단어들의 작은 모음을 포함하는 벡터를 얻으면, 이를 특징으로 변환해야 합니다. 각 특징은 단어 쌍입니다. 어텐션 마스킹은 필요한 원자재를 제공하지만, 이러한 단어 쌍 특징을 구축하지는 않습니다. 이를 위해 단일 계층 완전 연결 신경망을 사용할 수 있습니다.

신경망 계층이 이러한 쌍을 어떻게 생성할 수 있는지 알아보기 위해 직접 하나를 만들 것입니다. 인위적으로 명확하고 단순화될 것이며, 그 가중치는 실제 가중치와는 직접적인 관련이 없지만, 신경망이 이러한 두 단어 쌍 특징을 구축하는 데 필요한 표현력을 가지고 있음을 보여줄 것입니다. 작고 깔끔하게 유지하기 위해 이 예시에서 참석한 세 단어인 _battery_, _program_, _ran_ 에만 집중할 것입니다.

![](/posts/Transformers-from-Scratch/feature_creation_layer.png)

위 계층 다이어그램에서 가중치가 각 단어의 존재와 부재를 특징 모음으로 결합하는 방법을 볼 수 있습니다. 이것은 행렬 형태로도 표현될 수 있습니다.

![](/posts/Transformers-from-Scratch/feature_creation_matrix.png)

그리고 이것은 지금까지 본 단어들의 모음을 나타내는 벡터와 행렬 곱셈으로 계산될 수 있습니다.

![](/posts/Transformers-from-Scratch/second_order_feature_battery.png)

_battery_ 와 _ran_ 요소는 1이고 _program_ 요소는 0입니다. _bias_ 요소는 항상 1이며, 신경망의 특징입니다. 행렬 곱셈을 수행하면 _battery, ran_ 을 나타내는 요소에 대해 1이, _program, ran_ 을 나타내는 요소에 대해 -1이 나옵니다. 다른 경우에 대한 결과도 유사합니다.

![](/posts/Transformers-from-Scratch/second_order_feature_program.png)

이러한 단어 조합 특징을 계산하는 마지막 단계는 ReLU(Rectified Linear Unit) 비선형성을 적용하는 것입니다. 이것의 효과는 모든 음수 값을 0으로 대체하는 것입니다. 이것은 두 결과 모두를 정리하여 각 단어 조합 특징의 존재(1) 또는 부재(0)를 나타내도록 합니다.

이러한 복잡한 과정을 거쳐, 우리는 마침내 다중 단어 특징을 생성하는 행렬 곱셈 기반 방법을 갖게 되었습니다. 원래는 이들이 가장 최근 단어와 이전 단어 하나로 구성된다고 주장했지만, 이 방법을 자세히 살펴보면 다른 특징도 구축할 수 있음을 알 수 있습니다. 특징 생성 행렬이 하드 코딩되는 대신 학습되면 다른 구조를 학습할 수 있습니다. 이 장난감 예시에서도 _battery, program, ran_ 과 같은 세 단어 조합을 생성하는 것을 막을 수 없습니다. 이 조합이 충분히 자주 발생한다면 아마 표현될 것입니다. 단어의 순서가 어떻게 나타났는지 나타낼 방법은 없겠지만(적어도 [아직은](https://e2eml.school/#positional_encoding "null") 아님), 우리는 그들의 동시 발생을 사용하여 예측을 할 수 있습니다. 가장 최근 단어를 무시하는 _battery, program_ 과 같은 단어 조합을 사용하는 것도 가능할 것입니다. 이러한 유형의 특징과 다른 유형의 특징은 실제로는 생성될 가능성이 높으며, 트랜스포머가 선택적 2차 건너뛰기 시퀀스 모델이라는 주장이 지나치게 단순화되었음을 드러냅니다. 그보다 더 미묘한 차이가 있으며, 이제 여러분은 그 미묘한 차이가 무엇인지 정확히 알 수 있습니다. 이야기를 더 미묘하게 만들기 위해 바꾸는 것이 이번이 마지막은 아닐 것입니다.

이 형태에서 다중 단어 특징 행렬은 [위에서](https://e2eml.school/second_order_skips "null") 개발한 건너뛰기를 포함한 2차 시퀀스 모델인 한 번 더 행렬 곱셈을 할 준비가 되었습니다. 종합적으로,

- 특징 생성 행렬 곱셈,
- ReLU 비선형성, 그리고
- 전이 행렬 곱셈

의 시퀀스는 어텐션이 적용된 후 적용되는 피드포워드 처리 단계입니다. 논문의 방정식 2는 이러한 단계를 간결한 수학적 공식으로 보여줍니다.

![](/posts/Transformers-from-Scratch/feedforward_equations.png)

논문의 그림 1 아키텍처 다이어그램은 이들을 피드포워드 블록으로 묶어 보여줍니다.

![](/posts/Transformers-from-Scratch/architecture_feedforward.png)

### 시퀀스 완성 (Sequence completion)

지금까지 우리는 다음 단어 예측에 대해서만 이야기했습니다. 디코더가 긴 시퀀스를 생성하도록 하려면 몇 가지 추가해야 할 부분이 있습니다. 첫 번째는 **프롬프트(prompt)** 입니다. 트랜스포머가 나머지 시퀀스를 구축하는 데 시작점과 문맥을 제공하는 예시 텍스트입니다. 위 이미지의 오른쪽 열인 디코더로 입력되며, "Outputs (shifted right)"로 표시되어 있습니다. 흥미로운 시퀀스를 제공하는 프롬프트를 선택하는 것은 그 자체로 예술이며, 프롬프트 엔지니어링이라고 불립니다. 또한 알고리즘을 지원하기 위해 인간이 행동을 수정하는 훌륭한 예시이기도 합니다.

디코더가 시작할 부분 시퀀스를 얻으면, 순방향 패스를 수행합니다. 최종 결과는 단어의 예측 확률 분포 집합이며, 시퀀스의 각 위치에 대한 하나의 확률 분포입니다. 각 위치에서 분포는 어휘의 각 다음 단어에 대한 예측 확률을 보여줍니다. 우리는 시퀀스의 각 확정된 단어에 대한 예측 확률에는 관심이 없습니다. 이들은 이미 확정되었습니다. 우리가 정말 관심 있는 것은 프롬프트 끝 다음 단어에 대한 예측 확률입니다. 그 단어가 무엇이어야 하는지 선택하는 방법은 여러 가지가 있지만, 가장 간단한 방법은 **탐욕적(greedy)** 이라고 불리며, 가장 높은 확률을 가진 단어를 선택하는 것입니다.

새로운 다음 단어는 시퀀스에 추가되고, 디코더 하단의 "Outputs"에 대체되어 삽입되며, 지칠 때까지 이 과정이 반복됩니다.

자세히 설명할 준비가 아직 되지 않은 한 가지 부분은 또 다른 형태의 마스킹입니다. 트랜스포머가 예측을 할 때 미래가 아닌 과거만 보도록 보장하는 것입니다. 이것은 "Masked Multi-Head Attention"이라는 블록에 적용됩니다. 나중에 이것이 어떻게 이루어지는지 더 명확하게 설명할 수 있을 때 다시 다룰 것입니다.

### 임베딩 (Embeddings)

지금까지 설명한 대로 트랜스포머는 너무 큽니다. 예를 들어 어휘 크기 N이 50,000이라면, 모든 단어 쌍과 모든 잠재적인 다음 단어 사이의 전이 행렬은 50,000개의 열과 50,000의 제곱(25억) 개의 행을 가지며, 총 100조 개 이상의 요소를 가질 것입니다. 이는 현대 하드웨어로도 여전히 무리입니다.

문제는 행렬의 크기만이 아닙니다. 안정적인 전이 언어 모델을 구축하려면, 모든 잠재적인 시퀀스를 적어도 여러 번 보여주는 훈련 데이터를 제공해야 합니다. 이는 가장 야심찬 훈련 데이터 세트의 용량도 훨씬 초과할 것입니다.

다행히도 이 두 가지 문제에 대한 해결책인 **임베딩(embeddings)** 이 있습니다.

언어의 원-핫 표현에서는 각 단어에 대한 하나의 벡터 요소가 있습니다. 어휘 크기 N의 경우 해당 벡터는 N차원 공간입니다. 각 단어는 그 공간의 한 점을 나타내며, 여러 축 중 하나를 따라 원점에서 한 단위 떨어져 있습니다. 고차원 공간을 그리는 좋은 방법을 아직 찾지 못했지만, 아래에 대략적인 표현이 있습니다.

![](/posts/Transformers-from-Scratch/one_hot_vs_embedding.png)

임베딩에서는 이러한 단어 점들이 모두 재배열(선형 대수학 용어로 **투영(projected)**)되어 저차원 공간으로 이동합니다. 위 그림은 예를 들어 2차원 공간에서 어떻게 보일 수 있는지 보여줍니다. 이제 단어를 지정하는 데 N개의 숫자가 필요한 대신, 2개만 필요합니다. 이들은 새 공간에서 각 점의 (x,y) 좌표입니다. 다음은 우리의 장난감 예시에 대한 2차원 임베딩과 몇몇 단어의 좌표입니다.

![](/posts/Transformers-from-Scratch/embedded_words.png)

좋은 임베딩은 유사한 의미를 가진 단어들을 함께 그룹화합니다. 임베딩과 함께 작동하는 모델은 임베딩된 공간에서 패턴을 학습합니다. 이는 한 단어에 대해 학습한 모든 것이 자동으로 그 옆에 있는 모든 단어에 적용된다는 것을 의미합니다. 이는 필요한 훈련 데이터의 양을 줄이는 추가적인 이점을 가집니다. 각 예시는 단어의 전체 이웃에 적용되는 약간의 학습을 제공합니다.

이 그림에서는 중요한 구성 요소(_battery_, _log_, _program_)를 한 영역에, 전치사(_down_, _out_)를 다른 영역에, 동사(_check_, _find_, _ran_)를 중앙 근처에 배치하여 이를 보여주려고 했습니다. 실제 임베딩에서는 그룹화가 그렇게 명확하거나 직관적이지 않을 수 있지만, 기본 개념은 동일합니다. 유사하게 행동하는 단어들 사이의 거리는 작습니다.

임베딩은 필요한 매개변수의 수를 엄청나게 줄입니다. 그러나 임베딩된 공간의 차원이 적을수록 원본 단어에 대한 정보가 더 많이 버려집니다. 언어의 풍부함은 여전히 모든 중요한 개념을 서로 방해하지 않도록 배치하기 위해 상당한 공간을 필요로 합니다. 임베딩된 공간의 크기를 선택함으로써, 우리는 계산 부하와 모델 정확도 사이에서 균형을 맞출 수 있습니다.

단어를 원-핫 표현에서 임베딩된 공간으로 투영하는 것이 행렬 곱셈을 포함한다는 사실에 놀라지 않을 것입니다. 투영은 행렬이 가장 잘하는 일입니다. 하나의 행과 N개의 열을 가진 원-핫 행렬로 시작하여 2차원 임베딩 공간으로 이동하면, 투영 행렬은 N개의 행과 2개의 열을 가질 것입니다. 다음은 그 예시입니다.

![](/posts/Transformers-from-Scratch/embedding_projection.png)

이 예시는 _battery_ 를 나타내는 원-핫 벡터가 어떻게 해당 행을 추출하는지 보여줍니다. 이 행에는 임베딩된 공간에서 단어의 좌표가 포함됩니다. 관계를 더 명확하게 하기 위해 원-핫 벡터의 0은 숨겨져 있으며, 투영 행렬에서 추출되지 않는 다른 모든 행도 숨겨져 있습니다. 전체 투영 행렬은 밀집되어 있으며, 각 행에는 해당 단어의 좌표가 포함됩니다.

투영 행렬은 원본 원-핫 어휘 벡터 모음을 원하는 차원의 공간에서 어떤 구성으로든 변환할 수 있습니다. 가장 큰 트릭은 유용한 투영, 즉 유사한 단어들이 함께 그룹화되고, 서로 겹치지 않도록 충분한 차원을 가진 투영을 찾는 것입니다. 영어와 같은 일반적인 언어에 대한 괜찮은 사전 계산된 임베딩이 있습니다. 또한, 트랜스포머의 다른 모든 것과 마찬가지로 훈련 중에 학습될 수 있습니다.

원본 논문의 그림 1 아키텍처 다이어그램에서 임베딩이 일어나는 곳은 다음과 같습니다.

![](/posts/Transformers-from-Scratch/architecture_embedding.png)

### 위치 인코딩 (Positional encoding)

지금까지 우리는 단어의 위치가 무시된다고 가정했습니다. 적어도 가장 최근 단어 이전에 오는 단어들에 대해서는 말이죠. 이제 **위치 임베딩(positional embedding)** 을 사용하여 이를 수정할 것입니다.

단어의 임베딩된 표현에 위치 정보를 도입하는 방법은 여러 가지가 있지만, 원본 트랜스포머에서 사용된 방법은 원형 교란을 추가하는 것이었습니다.

![](/posts/Transformers-from-Scratch/positional_encoding.png)

임베딩 공간에서 단어의 위치는 원의 중심 역할을 합니다. 시퀀스에서 단어가 어디에 위치하는지에 따라 교란이 추가됩니다. 각 위치에 대해 단어는 같은 거리만큼 이동하지만 다른 각도로 이동하여, 시퀀스를 통해 이동할 때 원형 패턴을 만듭니다. 시퀀스에서 서로 가까운 단어들은 유사한 교란을 가지지만, 멀리 떨어진 단어들은 다른 방향으로 교란됩니다.

원은 2차원 도형이므로, 원형 흔들림을 표현하려면 임베딩 공간의 두 차원을 수정해야 합니다. 임베딩 공간이 두 차원보다 많다면(거의 항상 그렇습니다), 원형 흔들림은 다른 모든 차원 쌍에서 반복되지만, 다른 각 주파수로, 즉 각 경우에 다른 수의 회전을 쓸어냅니다. 일부 차원 쌍에서는 흔들림이 원의 많은 회전을 쓸어낼 것입니다. 다른 쌍에서는 회전의 작은 부분만 쓸어낼 것입니다. 다른 주파수의 이러한 모든 원형 흔들림의 조합은 시퀀스 내에서 단어의 절대 위치를 잘 표현합니다.

이것이 왜 작동하는지에 대한 직관을 아직 개발 중입니다. 단어와 어텐션 간의 학습된 관계를 방해하지 않는 방식으로 위치 정보를 혼합에 추가하는 것처럼 보입니다. 수학 및 함의에 대한 더 깊은 이해를 원하시면 Amirhossein Kazemnejad의 위치 인코딩 [튜토리얼](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/ "null")을 추천합니다.

표준 아키텍처 다이어그램에서 이 블록들은 위치 코드의 생성과 임베딩된 단어에 대한 추가를 보여줍니다.

![](/posts/Transformers-from-Scratch/architecture_positional.png)

### 디-임베딩 (De-embeddings)

단어를 임베딩하면 훨씬 더 효율적으로 작업할 수 있지만, 작업이 끝나면 원본 어휘의 단어로 다시 변환해야 합니다. 디-임베딩은 임베딩과 동일한 방식으로, 즉 한 공간에서 다른 공간으로의 투영, 다시 말해 행렬 곱셈으로 수행됩니다.

디-임베딩 행렬은 임베딩 행렬과 같은 모양이지만, 행과 열의 수가 뒤바뀌어 있습니다. 행 수는 변환하려는 공간의 차원입니다. 우리가 사용해 온 예시에서는 임베딩 공간의 크기인 2입니다. 열 수는 변환하려는 공간의 차원입니다. 전체 어휘의 원-핫 표현 크기인 우리 예시에서는 13입니다.

![](/posts/Transformers-from-Scratch/de_embedding.png)

좋은 디-임베딩 행렬의 값은 임베딩 행렬의 값만큼 직관적으로 설명하기 어렵지만, 효과는 유사합니다. 예를 들어 _program_ 단어를 나타내는 임베딩된 벡터가 디-임베딩 행렬과 곱해지면, 해당 위치의 값은 높아집니다. 그러나 고차원 공간으로의 투영이 작동하는 방식 때문에 다른 단어와 관련된 값은 0이 되지 않을 것입니다. 임베딩된 공간에서 _program_ 에 가장 가까운 단어들도 중간 정도의 높은 값을 가질 것입니다. 다른 단어들은 거의 0에 가까운 값을 가질 것입니다. 그리고 음수 값을 가진 단어들도 많이 있을 것입니다. 어휘 공간의 출력 벡터는 더 이상 원-핫이거나 희소하지 않을 것입니다. 거의 모든 값이 0이 아닌 밀집된 형태가 될 것입니다.

![](/posts/Transformers-from-Scratch/de_embedded_results.png)

괜찮습니다. 가장 높은 값과 관련된 단어를 선택하여 원-핫 벡터를 다시 만들 수 있습니다. 이 연산은 **argmax**라고도 불리며, 최대 값을 주는 인자(요소)를 의미합니다. 이것이 [위에서](https://e2eml.school/#sequence_completion "null") 언급된 탐욕적 시퀀스 완성을 수행하는 방법입니다. 훌륭한 첫 번째 단계이지만, 더 잘할 수 있습니다.

임베딩이 여러 단어에 매우 잘 매핑된다면, 우리는 항상 가장 좋은 단어를 선택하고 싶지 않을 수도 있습니다. 다른 단어들보다 아주 약간 더 나은 선택일 수 있으며, 약간의 다양성을 추가하면 결과가 더 흥미로워질 수 있습니다. 또한, 때로는 몇 단어 앞을 내다보고 최종 선택을 하기 전에 문장이 어떤 방향으로 갈 수 있는지 모두 고려하는 것이 유용합니다. 이를 위해서는 먼저 디-임베딩 결과를 확률 분포로 변환해야 합니다.

#### 소프트맥스 (Softmax)

argmax 함수는 가장 높은 값이 이기는 "하드"한 함수입니다. 다른 값들보다 아주 약간만 크더라도 말이죠. 여러 가능성을 동시에 고려하고 싶다면, **소프트맥스(softmax)** 에서 얻을 수 있는 "소프트" 최대 함수를 사용하는 것이 좋습니다. 벡터에서 값 x의 소프트맥스를 얻으려면, x의 지수, $e^x$를 벡터의 모든 값의 지수 합으로 나눕니다.

소프트맥스는 세 가지 이유로 여기에서 유용합니다. 첫째, 디-임베딩 결과 벡터를 임의의 값 집합에서 확률 분포로 변환합니다. 확률로서, 다른 단어가 선택될 가능성을 비교하기가 더 쉬워지고, 미래를 더 멀리 내다보고 싶다면 다중 단어 시퀀스의 가능성까지 비교할 수 있습니다.

둘째, 상위권 근처의 값들을 명확하게 구분합니다. 한 단어가 다른 단어보다 분명히 높은 점수를 얻으면, 소프트맥스는 그 차이를 과장하여 거의 argmax처럼 보이게 만들며, 우승 값은 1에 가깝고 다른 모든 값은 0에 가깝게 만듭니다. 그러나 여러 단어가 모두 상위권에 가깝게 나오면, 인위적으로 근접한 2위 결과를 짓밟지 않고 모두 높은 확률로 보존합니다.

셋째, 소프트맥스는 미분 가능합니다. 즉, 입력 요소의 작은 변화가 주어졌을 때 결과의 각 요소가 얼마나 변할지 계산할 수 있습니다. 이를 통해 역전파와 함께 사용하여 트랜스포머를 훈련할 수 있습니다。

소프트맥스 이해를 깊이 파고들고 싶다면 (또는 밤에 잠이 오지 않는다면) 이에 대한 더 완전한 [게시물](https://e2eml.school/softmax "null")이 있습니다.

디-임베딩 변환(아래 Linear 블록으로 표시)과 소프트맥스 함수가 함께 디-임베딩 프로세스를 완료합니다.

![](/posts/Transformers-from-Scratch/architecture_de_embedding.png)

### 멀티-헤드 어텐션 (Multi-head attention)

이제 투영(행렬 곱셈)과 공간(벡터 크기)의 개념에 익숙해졌으니, 새로운 활력으로 핵심 어텐션 메커니즘을 다시 살펴볼 수 있습니다. 각 단계에서 행렬의 모양을 더 구체적으로 설명하면 알고리즘을 명확히 하는 데 도움이 될 것입니다. 이를 위한 몇 가지 중요한 숫자가 있습니다.

- N: 어휘 크기. 우리 예시에서는 13입니다. 일반적으로 수만 개입니다.
- n: 최대 시퀀스 길이. 우리 예시에서는 12입니다. 논문에서는 수백 개 정도입니다. (명시하지 않음) GPT-3에서는 2048입니다.
- $d_{model}$​: 모델 전체에서 사용되는 임베딩 공간의 차원 수. 논문에서는 512입니다.

원본 입력 행렬은 문장의 각 단어를 원-핫 표현으로 가져와 각 원-핫 벡터가 자체 행이 되도록 쌓아 구성됩니다. 결과 입력 행렬은 n개의 행과 N개의 열을 가지며, 이를 $[n \times N]$로 줄여서 표기할 수 있습니다.

![](/posts/Transformers-from-Scratch/matrix_multiply_shape.png)

이전에 설명했듯이, 임베딩 행렬은 N개의 행과 $d_{model}$개의 열을 가지며, 이를 $[N \times d_{model}]$로 줄여서 표기할 수 있습니다. 두 행렬을 곱할 때, 결과는 첫 번째 행렬에서 행 수를 가져오고, 두 번째 행렬에서 열 수를 가져옵니다. 이렇게 하면 임베딩된 단어 시퀀스 행렬은  $[N \times d_{model}]$ 모양을 가집니다.

트랜스포머를 통해 행렬 모양의 변화를 추적하여 무엇이 진행되고 있는지 확인할 수 있습니다. 초기 임베딩 후, 위치 인코딩은 곱셈이 아닌 덧셈이므로 모양을 변경하지 않습니다. 그런 다음 임베딩된 단어 시퀀스는 어텐션 계층으로 들어가서 같은 모양으로 나옵니다. (내부 작동 방식은 잠시 후에 다시 다룰 것입니다.) 마지막으로, 디-임베딩은 행렬을 원래 모양으로 복원하여 시퀀스의 모든 위치에서 어휘의 모든 단어에 대한 확률을 제공합니다.

![](/posts/Transformers-from-Scratch/matrix_shapes.png)

#### 왜 하나 이상의 어텐션 헤드가 필요한가 (Why we need more than one attention head)

이제 어텐션 메커니즘을 설명하는 첫 번째 단계에서 제가 했던 단순한 가정 중 일부에 직면할 시간입니다. 단어는 원-핫 벡터가 아닌 밀집된 임베딩 벡터로 표현됩니다. 어텐션은 단순히 1 또는 0, 켜짐 또는 꺼짐이 아니라 그 사이의 어느 값도 가질 수 있습니다. 결과를 0과 1 사이에 오도록 하려면 소프트맥스 트릭을 다시 사용합니다. 이는 모든 값을 $[0, 1]$ 어텐션 범위에 강제로 놓이게 하는 이중 이점을 가지며, 가장 높은 값을 강조하고 가장 작은 값을 적극적으로 압축하는 데 도움이 됩니다. 이는 모델의 최종 출력을 해석할 때 이전에 활용했던 미분 가능한 거의-argmax 동작입니다.

어텐션에 소프트맥스 함수를 넣는 복잡한 결과 중 하나는 단일 요소에 집중하는 경향이 있다는 것입니다. 이는 이전에 없었던 한계입니다. 때로는 다음 단어를 예측할 때 여러 선행 단어를 염두에 두는 것이 유용하지만, 소프트맥스는 우리에게서 그것을 빼앗아갔습니다. 이것은 모델에 문제가 됩니다.

해결책은 여러 다른 어텐션 인스턴스, 즉 **헤드(heads)** 를 동시에 실행하는 것입니다. 이를 통해 트랜스포머는 다음을 예측할 때 여러 이전 단어를 동시에 고려할 수 있습니다. 이는 소프트맥스를 도입하기 전에 우리가 가졌던 힘을 되찾아줍니다.

불행히도, 이렇게 하면 계산 부하가 정말로 증가합니다. 어텐션을 계산하는 것이 이미 작업의 대부분이었는데, 이제 우리가 사용하고 싶은 헤드 수만큼 곱했습니다. 이를 해결하기 위해 모든 것을 저차원 임베딩 공간으로 투영하는 트릭을 다시 사용할 수 있습니다. 이렇게 하면 관련된 행렬이 축소되어 계산 시간이 크게 줄어듭니다. 문제가 해결되었습니다.

이것이 어떻게 전개되는지 보기 위해 행렬 모양을 계속 살펴볼 수 있습니다. 멀티헤드 어텐션 블록의 분기와 엮임을 통해 행렬 모양을 추적하려면 세 가지 숫자가 더 필요합니다.

- $d_k$​: 키와 쿼리에 사용되는 임베딩 공간의 차원. 논문에서는 64입니다.
- $d_v$​: 값에 사용되는 임베딩 공간의 차원. 논문에서는 64입니다.
- h: 헤드 수. 논문에서는 8입니다.

![](/posts/Transformers-from-Scratch/architecture_multihead.png)

임베딩된 단어들로 구성된 $[n \times d_{model}]$ 시퀀스는 이어지는 모든 작업의 기반이 됩니다. 각 경우, $W_v$, $W_q$, 그리고 $W_k$ (모두 아키텍처 다이어그램에서 "Linear" 블록으로만 표시되어 있음)라는 행렬이 원래 임베딩된 단어 시퀀스를 값(values) 행렬 **V**, 쿼리(queries) 행렬 **Q**, 그리고 키(keys) 행렬 **K**로 변환합니다. **K**와 **Q**는 $[n \times d_k]$로 동일한 형태를 가지지만, **V**는 $[n \times d_v]$로 다를 수 있습니다. 논문에서는 $d_k$​와 $d_v​$가 동일해서 약간 혼동스러울 수 있지만, 반드시 같아야 하는 것은 아닙니다. 이 설정의 중요한 측면은 각 어텐션 헤드가 자신만의 Wv, Wq, Wk 변환을 갖는다는 것입니다. 이는 각 헤드가 임베딩 공간에서 초점을 맞추고자 하는 부분을 확대하고 확장할 수 있으며, 이는 다른 헤드들이 초점을 맞추는 부분과 다를 수 있다는 것을 의미합니다.

각 어텐션 헤드의 결과는 **V**와 동일한 형태를 가집니다. 이제 우리는 시퀀스의 서로 다른 요소에 집중하는 _**h**_ 개의 서로 다른 결과 벡터를 갖게 되는 문제가 발생합니다. 이 벡터들을 하나로 결합하기 위해 우리는 선형 대수의 힘을 활용하여 이 모든 결과를 하나의 거대한 $[n \times h * d_v]$ 행렬로 단순히 연결(concatenate)합니다. 그런 다음, 시작했던 것과 동일한 형태로 끝나도록 $[h * d_v \times d_{model}$ 형태의 변환을 한 번 더 사용합니다.

이 모든 것을 간결하게 표현하면 다음과 같습니다.

![](/posts/Transformers-from-Scratch/multihead_attention_equation.png)

### 다시 살펴보는 단일 헤드 어텐션 (Single head attention revisited)

우리는 이미 [위에서](https://e2eml.school/#attention "null") 어텐션의 개념적 그림을 살펴보았습니다. 실제 구현은 약간 더 복잡하지만, 우리의 이전 직관은 여전히 유용합니다. 쿼리와 키는 더 이상 쉽게 검사하고 해석할 수 없습니다. 모두 고유한 하위 공간으로 투영되기 때문입니다. 우리의 개념적 그림에서 쿼리 행렬의 한 행은 어휘 공간의 한 점을 나타내며, 이는 원-핫 표현 덕분에 단 하나만 나타냅니다. 임베딩된 형태에서 쿼리 행렬의 한 행은 임베딩된 공간의 한 점을 나타내며, 이는 유사한 의미와 용법을 가진 단어 그룹 근처에 있을 것입니다. 개념적 그림은 하나의 쿼리 단어를 키 집합에 매핑하며, 이는 다시 집중되지 않는 모든 값을 필터링합니다. 실제 구현에서 각 어텐션 헤드는 쿼리 단어를 또 다른 저차원 임베딩 공간의 한 점에 매핑합니다. 이 결과로 어텐션은 개별 단어 간의 관계가 아닌 단어 그룹 간의 관계가 됩니다. 이는 의미적 유사성(임베딩된 공간에서의 근접성)을 활용하여 유사한 단어에 대해 학습한 것을 일반화하는 능력을 얻습니다.

어텐션 계산을 통해 행렬의 모양을 추적하는 것은 무엇을 하는지 추적하는 데 도움이 됩니다.

![](/posts/Transformers-from-Scratch/architecture_single_head.png)

쿼리와 키 행렬 **Q**와 **K**는 모두 $[n \times d_k]$의 형태로 들어옵니다. **K**가 곱셈 전에 전치되기 때문에, $QK^T$의 결과는 $[n \times d_k​] * [d_k​ \times n]=[n \times n]$ 형태의 행렬이 됩니다. 이 행렬의 모든 원소를 $\sqrt{d_k}$로 나누는 것은 값의 크기가 지나치게 커지는 것을 방지하고, 역전파가 잘 수행되도록 돕는 것으로 알려져 있습니다. 앞서 언급했듯이, 소프트맥스는 결과를 근사적인 argmax 형태로 만들어 시퀀스의 다른 요소들보다 한 요소에 더 집중하도록 만듭니다. 이 형태의 $[n \times n]$ 어텐션 행렬은 시퀀스의 각 요소가 다음 요소를 예측하기 위한 가장 관련성 높은 문맥을 얻기 위해 무엇을 봐야 하는지를 나타내며, 시퀀스의 각 요소를 시퀀스의 다른 한 요소에 대략적으로 매핑합니다. 이것은 최종적으로 값 행렬 **V**에 적용되는 필터로, 선택된 값들만 남깁니다. 그 결과, 시퀀스에서 이전에 나온 대부분의 정보를 무시하고, 가장 유용한 하나의 이전 요소에만 초점을 맞추는 효과를 줍니다.

![](/posts/Transformers-from-Scratch/attention_equation.png)

이 계산 집합을 이해하는 한 가지 까다로운 부분은 이것이 가장 최근 단어뿐만 아니라 입력 시퀀스의 모든 요소, 즉 문장의 모든 단어에 대한 어텐션을 계산하고 있다는 점을 명심하는 것입니다. 또한 이전 단어에 대한 어텐션도 계산합니다. 우리는 이러한 결과에 크게 주목하지 않습니다. 이미 다음 단어가 예측되고 확정되었기 때문입니다. 또한 미래 단어에 대한 어텐션도 계산합니다. 이들은 아직 멀리 떨어져 있고 즉각적인 선행 단어가 아직 선택되지 않았기 때문에 아직 큰 용도가 없습니다. 그러나 이러한 계산이 가장 최근 단어에 대한 어텐션에 영향을 미칠 수 있는 간접적인 경로가 있으므로, 우리는 이 모든 것을 포함합니다. 단지 마지막에 시퀀스의 모든 위치에 대한 단어 확률을 계산할 때, 우리는 대부분을 버리고 다음 단어에만 집중할 뿐입니다.

**마스크(Mask) 블록**은 적어도 이 시퀀스 완성 작업에서는 미래를 들여다볼 수 없도록 하는 제약 조건을 적용합니다. 이로써 상상 속의 미래 단어들로 인해 발생하는 이상한 인공물을 방지합니다. 마스크는 간단하고 효과적인 방식으로 작동하는데, 현재 위치 이후의 모든 단어에 대한 어텐션(attention) 값을 수동으로 음의 무한대(−∞)로 설정합니다. 'The Annotated Transformer'라는, 논문의 파이썬(Python) 구현을 한 줄 한 줄 보여주는 매우 유용한 자료에서는 마스크 행렬이 시각화되어 있습니다. 보라색 셀은 어텐션이 허용되지 않는 부분을 나타냅니다. 각 행은 시퀀스의 한 요소를 나타냅니다. 첫 번째 행은 자신(첫 번째 요소)에게만 어텐션이 허용되고, 그 이후의 어떤 것에도 허용되지 않습니다. 마지막 행은 자신(마지막 요소)과 그 이전에 있는 모든 것에 어텐션이 허용됩니다. 마스크는 $[n \times n]$ 행렬입니다. 이 행렬은 행렬 곱셈이 아닌, 보다 간단한 요소별 곱셈을 통해 적용됩니다. 이로 인해 어텐션 행렬에서 마스크의 보라색 요소에 해당하는 모든 값을 수동으로 음의 무한대로 설정하는 효과가 발생합니다.

![](/posts/Transformers-from-Scratch/mask.png)

어텐션이 구현되는 방식의 또 다른 중요한 차이점은 시퀀스에서 단어가 제시되는 순서를 활용하고, 어텐션을 단어 대 단어 관계가 아닌 위치 대 위치 관계로 나타낸다는 것입니다. 이는 $[n \times n]$ 모양에서 분명합니다. 이는 행 인덱스로 표시된 시퀀스의 각 요소를 열 인덱스로 표시된 시퀀스의 다른 요소(들)에 매핑합니다. 이는 임베딩 공간에서 작동하므로 무엇을 하고 있는지 더 쉽게 시각화하고 해석하는 데 도움이 됩니다. 쿼리와 키 사이의 관계를 나타내기 위해 임베딩 공간에서 가까운 단어를 찾는 추가 단계를 생략합니다.

### 스킵 연결 (Skip connection)

어텐션은 트랜스포머가 하는 일의 가장 근본적인 부분입니다. 그것은 핵심 메커니즘이며, 우리는 이제 꽤 구체적인 수준으로 그것을 탐색했습니다. 이제부터는 그것이 잘 작동하도록 만드는 데 필요한 나머지 배관입니다. 그것은 어텐션이 우리의 무거운 작업 부하를 끌어낼 수 있도록 하는 나머지 장치입니다.

아직 설명하지 않은 한 가지는 **스킵 연결(skip connection)** 입니다. 이들은 "Add and Norm"으로 표시된 블록에서 멀티-헤드 어텐션 블록과 요소별 피드포워드 블록 주변에서 발생합니다. 스킵 연결에서는 입력의 복사본이 일련의 계산 결과에 추가됩니다. 어텐션 블록의 입력은 그 출력에 다시 추가됩니다. 요소별 피드포워드 블록의 입력은 그 출력에 추가됩니다.

![](/posts/Transformers-from-Scratch/architecture_add_norm.png)

스킵 연결은 두 가지 목적을 수행합니다.

첫째, 그래디언트를 부드럽게 유지하는 데 도움이 되며, 이는 역전파에 큰 도움이 됩니다. 어텐션은 필터이며, 이는 올바르게 작동할 때 통과하려는 대부분의 것을 차단한다는 것을 의미합니다. 이 결과로 많은 입력의 작은 변화가 차단된 채널에 떨어지면 출력에 큰 변화를 일으키지 않을 수 있습니다. 이는 그래디언트에서 평평하지만 여전히 계곡 바닥 근처에도 없는 데드 스팟을 생성합니다. 이러한 안장점과 능선은 역전파의 큰 걸림돌입니다. 스킵 연결은 이를 부드럽게 하는 데 도움이 됩니다. 어텐션의 경우, 모든 가중치가 0이고 모든 입력이 차단되더라도 스킵 연결은 입력의 복사본을 결과에 추가하고 입력의 작은 변화가 여전히 결과에 눈에 띄는 변화를 가져오도록 보장합니다. 이는 그래디언트 하강이 좋은 솔루션에서 멀리 떨어진 곳에 갇히는 것을 방지합니다.

스킵 연결은 ResNet 이미지 분류기 시대 이래로 성능을 향상시키는 방법으로 인기를 얻었습니다. 이제 신경망 아키텍처의 표준 기능입니다. 시각적으로, 스킵 연결이 있는 네트워크와 없는 네트워크를 비교하여 스킵 연결이 미치는 영향을 볼 수 있습니다. 이 [논문](https://arxiv.org/abs/1712.09913 "null")의 아래 그림은 스킵 연결이 있는 ResNet과 없는 ResNet을 보여줍니다. 스킵 연결을 사용하면 손실 함수 언덕의 기울기가 훨씬 더 완만하고 균일합니다. 작동 방식과 이유에 대해 더 깊이 파고들고 싶다면, 트랜스포머에 사용되는 계층 정규화의 가까운 사촌인 배치 정규화에 대한 더 자세한 [게시물](https://e2eml.school/batch_normalization "null")을 작성했습니다.

![](/posts/Transformers-from-Scratch/skip_connection_gradients.png)

스킵 연결의 두 번째 목적은 트랜스포머에 특화되어 있습니다. 즉, 원본 입력 시퀀스를 보존하는 것입니다. 많은 어텐션 헤드가 있더라도 단어가 자신의 위치에 주의를 기울일 것이라는 보장은 없습니다. 어텐션 필터가 관련성이 있을 수 있는 이전 단어를 모두 주시하기 위해 가장 최근 단어를 완전히 잊어버릴 수도 있습니다. 스킵 연결은 원본 단어를 다시 신호에 수동으로 추가하여, 단어가 누락되거나 잊혀질 방법이 없도록 합니다. 이러한 견고성의 원천은 트랜스포머가 다양한 시퀀스 완성 작업에서 좋은 성능을 보이는 이유 중 하나일 수 있습니다.

### 계층 정규화 (Layer normalization)

정규화는 스킵 연결과 잘 어울리는 단계입니다. 반드시 함께 가야 할 이유는 없지만, 어텐션이나 피드포워드 신경망과 같은 계산 그룹 뒤에 배치될 때 가장 잘 작동합니다.

계층 정규화의 간단한 설명은 행렬의 값이 평균이 0이 되도록 이동하고 표준 편차가 1이 되도록 스케일링된다는 것입니다.

![](/posts/Transformers-from-Scratch/normalization.png)

더 자세한 설명은 트랜스포머와 같은 시스템에서는 많은 움직이는 부분이 있고 그 중 일부는 행렬 곱셈(예: 소프트맥스 연산자 또는 ReLU)이 아닌 경우, 값의 크기와 양수와 음수 사이의 균형이 중요합니다. 모든 것이 선형이라면 모든 입력을 두 배로 늘리면 출력이 두 배로 커지고 모든 것이 잘 작동합니다. 신경망은 그렇지 않습니다. 신경망은 본질적으로 비선형적이므로 매우 표현력이 뛰어나지만 신호의 크기와 분포에 민감합니다. 정규화는 다층 신경망 전체의 각 단계에서 신호 값의 일관된 분포를 유지하는 데 유용하다고 입증된 기술입니다. 이는 매개변수 값의 수렴을 촉진하고 일반적으로 훨씬 더 나은 성능을 가져옵니다.

정규화는 그 효과에 대한 높은 수준의 설명에도 불구하고, 정확히 왜 그렇게 잘 작동하는지에 대해서는 아직 완전히 밝혀지지 않은 부분이 있습니다. 이 토끼굴보다 조금 더 깊이 들어가고 싶다면, 트랜스포머에 사용되는 계층 정규화의 가까운 사촌인 배치 정규화에 대한 더 자세한 [게시물](https://e2eml.school/batch_normalization "null")을 작성했습니다.

### 다중 레이어 (Multiple layers)

위에서 기초를 다질 때, 우리는 신중하게 선택된 가중치를 가진 어텐션 블록과 피드포워드 블록이 괜찮은 언어 모델을 만드는 데 충분하다는 것을 보여주었습니다. 우리 예시에서는 대부분의 가중치가 0이었고, 몇몇은 1이었으며, 모두 직접 선택했습니다. 원시 데이터에서 훈련할 때는 이러한 사치를 누릴 수 없을 것입니다. 처음에는 가중치가 모두 무작위로 선택되며, 대부분은 0에 가깝고, 그렇지 않은 몇몇은 아마도 우리가 필요한 것이 아닐 것입니다. 모델이 잘 작동하기 위해 필요한 곳과는 거리가 니다.

역전파를 통한 확률적 경사 하강법은 꽤 놀라운 일을 할 수 있지만, 운에 많이 의존합니다. 정답에 도달하는 단 한 가지 방법, 즉 네트워크가 잘 작동하는 데 필요한 단 하나의 가중치 조합만 있다면, 그것을 찾을 가능성은 낮습니다. 그러나 좋은 해결책으로 가는 많은 경로가 있다면, 모델이 거기에 도달할 가능성은 훨씬 더 좋습니다.

단일 어텐션 계층(하나의 멀티-헤드 어텐션 블록과 하나의 피드포워드 블록)만으로는 좋은 트랜스포머 매개변수 집합으로 가는 단 하나의 경로만 허용합니다. 모든 행렬의 모든 요소는 모든 요소가 잘 작동하도록 올바른 값을 찾아야 합니다. 이는 취약하고 깨지기 쉬우며, 매개변수에 대한 초기 추측이 매우 운이 좋지 않으면 이상적이지 않은 해결책에 갇힐 가능성이 높습니다.

트랜스포머가 이 문제를 회피하는 방법은 여러 어텐션 계층을 갖는 것입니다. 각 계층은 이전 계층의 출력을 입력으로 사용합니다. 스킵 연결을 사용하면 전체 파이프라인이 개별 어텐션 블록이 실패하거나 이상한 결과를 내더라도 견고합니다. 여러 개를 갖는다는 것은 부족한 부분을 채워줄 다른 계층들이 기다리고 있다는 것을 의미합니다. 하나가 엉망이 되거나 어떤 식으로든 잠재력을 발휘하지 못하더라도, 다음 계층이 그 간극을 메우거나 오류를 수정할 또 다른 기회를 가질 것입니다. 논문에서는 더 많은 계층이 더 나은 성능을 가져왔지만, 6개 이후에는 개선이 미미해졌다고 밝혔습니다.

여러 계층을 생각하는 또 다른 방법은 컨베이어 벨트 조립 라인으로 생각하는 것입니다. 각 어텐션 블록과 피드포워드 블록은 라인에서 입력을 가져와 유용한 어텐션 행렬을 계산하고 다음 단어 예측을 할 기회를 가집니다. 그들이 생산하는 결과는 유용하든 아니든 컨베이어에 다시 추가되어 다음 계층으로 전달됩니다.

![](/posts/Transformers-from-Scratch/layer_conveyer.png)

이는 많은 계층의 신경망에 대한 전통적인 "깊은" 설명과는 대조적입니다. 스킵 연결 덕분에 연속적인 계층은 점점 더 정교한 추상화를 제공하기보다는 오히려 중복성을 제공합니다. 한 계층에서 놓친 어텐션을 집중하고 유용한 특징을 만들고 정확한 예측을 할 수 있는 모든 기회는 항상 다음 계층에서 잡을 수 있습니다. 계층은 조립 라인의 작업자가 되어 각자 할 수 있는 일을 하지만, 모든 조각을 잡는 것에 대해 걱정하지 않습니다. 다음 작업자가 놓친 것을 잡을 것이기 때문입니다.

### 디코더 스택 (Decoder stack)

지금까지 우리는 인코더 스택(트랜스포머 아키텍처의 왼쪽)을 신중하게 무시하고 디코더 스택(오른쪽)을 선호했습니다. 몇 단락 후에 이를 수정할 것입니다. 그러나 디코더만으로도 꽤 유용하다는 점에 주목할 가치가 있습니다.

[시퀀스 완성 작업 설명](https://e2eml.school/#sequence_completion "null")에서 설명했듯이, 디코더는 부분 시퀀스를 완성하고 원하는 만큼 확장할 수 있습니다. OpenAI는 이를 위해 GPT(generative pre-training) 모델 제품군을 만들었습니다. 이 [보고서](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf "null")에서 설명하는 아키텍처는 친숙하게 보일 것입니다. 인코더 스택과 모든 관련 연결이 제거된 트랜스포머입니다. 남은 것은 12개 계층의 디코더 스택입니다.

![](/posts/Transformers-from-Scratch/gpt_architecture.png)

[BERT](https://arxiv.org/pdf/1810.04805v2.pdf "null"), [ELMo](https://arxiv.org/abs/1802.05365 "null"), [Copilot](https://copilot.github.com/ "null")과 같은 생성형 모델을 접할 때마다 아마도 트랜스포머의 디코더 절반이 작동하는 것을 보고 있을 것입니다.

### 인코더 스택 (Encoder stack)

디코더에 대해 배운 거의 모든 것이 인코더에도 적용됩니다. 가장 큰 차이점은 성능의 옳고 그름을 판단하는 데 사용할 수 있는 명시적인 예측이 끝에 없다는 것입니다. 대신, 인코더 스택의 최종 결과물은 추상적인 형태를 띠게 됩니다. 즉, 임베딩된 공간의 벡터 시퀀스입니다. 이는 특정 언어나 어휘와 분리된 시퀀스의 순수한 의미론적 표현으로 묘사되었지만, 저에게는 지나치게 낭만적으로 들립니다. 우리가 확실히 아는 것은 그것이 디코더 스택에 의도와 의미를 전달하는 데 유용한 신호라는 것입니다.

인코더 스택을 갖는 것은 시퀀스를 생성하는 것뿐만 아니라 트랜스포머의 모든 잠재력을 열어줍니다. 이제 시퀀스를 한 언어에서 다른 언어로 번역(또는 변환)할 수 있습니다. 번역 작업에서 훈련하는 것은 시퀀스 완성 작업에서 훈련하는 것과 다릅니다. 훈련 데이터는 원본 언어의 시퀀스와 대상 언어의 일치하는 시퀀스 모두를 필요로 합니다. 원본 언어의 전체 문장은 인코더를 통해 실행되고(이번에는 마스킹 없음, 전체 문장을 본 후 번역을 생성한다고 가정하므로) 결과, 즉 최종 인코더 계층의 출력은 각 디코더 계층의 입력으로 제공됩니다. 그런 다음 디코더의 시퀀스 생성은 이전과 같이 진행되지만, 이번에는 시작할 프롬프트가 없습니다.

### 크로스-어텐션 (Cross-attention)

전체 트랜스포머를 완전히 작동시키는 마지막 단계는 인코더와 디코더 스택 사이의 연결인 **크로스-어텐션(cross-attention)** 블록입니다. 우리는 이를 마지막으로 남겨두었고, 우리가 다져놓은 기초 덕분에 설명할 것이 많지 않습니다.

크로스-어텐션은 키 행렬 **K**와 값 행렬 **V**가 이전 디코더 계층의 출력이 아닌 최종 인코더 계층의 출력에 기반한다는 점을 제외하고는 셀프-어텐션과 동일하게 작동합니다. 쿼리 행렬 **Q**는 여전히 이전 디코더 계층의 결과에서 계산됩니다. 이것이 원본 시퀀스의 정보가 대상 시퀀스로 전달되어 생성을 올바른 방향으로 유도하는 채널입니다. 동일한 임베딩된 원본 시퀀스가 디코더의 모든 계층에 제공된다는 점은 흥미롭습니다. 이는 연속적인 계층이 중복성을 제공하고 모두 동일한 작업을 수행하기 위해 협력한다는 개념을 뒷받침합니다.

![](/posts/Transformers-from-Scratch/architecture_cross_attention.png)

### 토큰화 (Tokenizing)

우리는 트랜스포머를 완전히 이해했습니다! 더 이상 신비한 블랙박스는 없을 정도로 충분히 자세히 다루었습니다. 우리가 깊이 파고들지 않은 몇 가지 구현 세부 사항이 있습니다. 직접 작동하는 버전을 구축하려면 이에 대해 알아야 할 것입니다. 이 마지막 몇 가지 정보는 트랜스포머가 어떻게 작동하는지에 대한 것이라기보다는 신경망이 잘 작동하도록 만드는 방법에 대한 것입니다. [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html "null")가 이러한 간극을 채우는 데 도움이 될 것입니다.

그러나 아직 완전히 끝나지 않았습니다. 데이터를 처음부터 어떻게 표현하는지에 대해 여전히 중요한 점들이 있습니다. 이것은 제가 매우 중요하게 생각하지만 소홀히 하기 쉬운 주제입니다. 알고리즘의 힘에 대한 것이라기보다는 데이터를 사려 깊게 해석하고 그 의미를 이해하는 것에 대한 것입니다.

우리는 어휘가 각 단어와 관련된 하나의 요소를 가진 고차원 원-핫 벡터로 표현될 수 있다고 언급했습니다. 이를 위해서는 우리가 표현할 단어가 정확히 몇 개인지, 그리고 무엇인지 알아야 합니다.

순진한 접근 방식은 웹스터 사전에서 찾을 수 있는 모든 가능한 단어 목록을 만드는 것입니다. 영어의 경우 수만 개가 될 것이며, 정확한 숫자는 우리가 포함하거나 제외하는 것에 따라 달라질 것입니다. 그러나 이것은 지나친 단순화입니다. 대부분의 단어는 복수형, 소유격, 활용형을 포함하여 여러 형태를 가집니다. 단어는 대체 철자를 가질 수 있습니다. 그리고 데이터가 매우 신중하게 정리되지 않았다면, 온갖 종류의 오타를 포함할 것입니다. 이는 자유 형식 텍스트, 신조어, 속어, 전문 용어, 그리고 방대한 유니코드 세계가 열어주는 가능성은 언급하지도 않았습니다. 모든 가능한 단어의 포괄적인 목록은 비현실적으로 길 것입니다.

합리적인 대안은 단어 대신 개별 문자를 빌딩 블록으로 사용하는 것입니다. 문자의 포괄적인 목록은 우리가 계산할 수 있는 용량 내에 있습니다. 그러나 여기에는 몇 가지 문제가 있습니다. 데이터를 임베딩 공간으로 변환한 후, 우리는 그 공간의 거리가 의미론적 해석을 가진다고 가정합니다. 즉, 서로 가까이 있는 점들은 유사한 의미를 가지고, 멀리 떨어진 점들은 매우 다른 의미를 가진다고 가정합니다. 이를 통해 우리는 한 단어에 대해 학습한 것을 즉각적인 이웃으로 암묵적으로 확장할 수 있습니다. 이는 계산 효율성을 위해 의존하는 가정이며, 트랜스포머가 일반화하는 능력을 얻는 원천입니다.

개별 문자 수준에서는 의미론적 내용이 거의 없습니다. 예를 들어 영어에는 한 글자 단어가 몇 개 있지만 많지는 않습니다. 이모지는 예외이지만, 대부분의 데이터 세트의 주요 내용은 아닙니다. 이는 도움이 되지 않는 임베딩 공간을 가지는 불행한 상황에 처하게 합니다.

이론적으로는 여전히 이를 해결할 수 있을 것입니다. 단어, 단어 어간 또는 단어 쌍과 같이 의미론적으로 유용한 시퀀스를 구축하기에 충분히 풍부한 문자 조합을 볼 수 있다면 말이죠. 불행히도, 트랜스포머가 내부적으로 생성하는 특징은 순서가 지정된 입력 집합보다는 입력 쌍의 모음처럼 작동합니다. 이는 단어의 표현이 문자 쌍의 모음이 될 것이며, 그 순서는 강력하게 표현되지 않는다는 것을 의미합니다. 트랜스포머는 계속해서 아나그램과 함께 작업해야 하므로 작업이 훨씬 더 어려워집니다. 그리고 실제로 문자 수준 표현에 대한 실험에서는 트랜스포머가 제대로 작동하지 않는 것으로 나타났습니다.

#### 바이트 쌍 인코딩 (Byte pair encoding)

다행히도 이에 대한 우아한 해결책이 있습니다. [바이트 쌍 인코딩(byte pair encoding)](https://en.m.wikipedia.org/wiki/Byte_pair_encoding "null")이라고 불립니다. 문자 수준 표현에서 시작하여 각 문자에 고유한 바이트 코드가 할당됩니다. 그런 다음 일부 대표 데이터를 스캔한 후 가장 일반적인 바이트 쌍이 함께 그룹화되어 새 바이트, 즉 새 코드가 할당됩니다. 이 새 코드는 데이터에 다시 대체되고, 이 과정이 반복됩니다.

문자 쌍을 나타내는 코드는 다른 문자 또는 문자 쌍을 나타내는 코드와 결합되어 더 긴 문자 시퀀스를 나타내는 새 코드를 얻을 수 있습니다. 코드가 나타낼 수 있는 문자 시퀀스의 길이에는 제한이 없습니다. 일반적으로 반복되는 시퀀스를 나타내는 데 필요한 만큼 길어질 것입니다. 바이트 쌍 인코딩의 멋진 점은 모든 가능한 시퀀스를 무작정 표현하는 것이 아니라, 데이터에서 어떤 긴 문자 시퀀스를 학습할지 추론한다는 것입니다. 예를 들어 _transformer_와 같은 긴 단어를 단일 바이트 코드로 표현하는 것을 학습하지만, _ksowjmckder_와 같이 비슷한 길이의 임의의 문자열에는 코드를 낭비하지 않을 것입니다. 그리고 단일 문자 빌딩 블록에 대한 모든 바이트 코드를 유지하므로, 이상한 오타, 새로운 단어, 심지어 외국어도 여전히 표현할 수 있습니다.

바이트 쌍 인코딩을 사용할 때 어휘 크기를 할당할 수 있으며, 해당 크기에 도달할 때까지 새 코드를 계속 구축합니다. 어휘 크기는 문자열이 텍스트의 의미론적 내용을 포착할 만큼 충분히 길어야 합니다. 의미가 있어야 합니다. 그래야 트랜스포머에 충분히 풍부한 정보를 제공할 수 있습니다.

바이트 쌍 인코더가 훈련되거나 빌려온 후, 우리는 데이터를 트랜스포머에 공급하기 전에 전처리하는 데 사용할 수 있습니다. 이는 끊어지지 않는 텍스트 스트림을 별개의 덩어리(대부분은 인식 가능한 단어이기를 바랍니다)의 시퀀스로 분할하고, 각 덩어리에 대한 간결한 코드를 제공합니다. 이것이 **토큰화(tokenization)** 라고 불리는 과정입니다.

### 오디오 입력 (Audio input)

이제 이 모든 모험을 시작할 때 우리의 원래 목표가 음성 명령의 오디오 신호를 텍스트 표현으로 번역하는 것이었음을 기억합시다. 지금까지 우리의 모든 예시는 우리가 쓰여진 언어의 문자와 단어를 다루고 있다는 가정하에 진행되었습니다. 이것을 오디오로도 확장할 수 있지만, 이는 신호 전처리에 대한 훨씬 더 대담한 탐험을 필요로 할 것입니다.

오디오 신호의 정보는 우리의 귀와 뇌가 음성을 이해하는 데 사용하는 부분을 추출하기 위해 강력한 전처리의 이점을 얻습니다. 이 방법은 멜-주파수 켑스트럼 필터링(Mel-frequency cepstrum filtering)이라고 불리며, 이름만큼이나 복잡합니다. 매혹적인 세부 사항을 파고들고 싶다면 잘 설명된 [튜토리얼](http://www.speech.cs.cmu.edu/15-492/slides/03_mfcc.pdf "null")이 있습니다.

전처리가 완료되면 원시 오디오는 벡터 시퀀스로 변환되며, 각 요소는 특정 주파수 범위에서 오디오 활동의 변화를 나타냅니다. 이는 밀집되어 있고(0인 요소 없음) 모든 요소는 실수 값을 가집니다.

긍정적인 측면은 각 벡터가 트랜스포머에 대한 좋은 "단어" 또는 토큰이 된다는 것입니다. 이는 어떤 의미를 가지기 때문입니다. 이는 단어의 일부로 인식 가능한 소리 집합으로 직접 변환될 수 있습니다.

다른 한편으로, 각 벡터를 단어로 취급하는 것은 이상합니다. 각 벡터가 고유하기 때문입니다. 소리의 미묘하게 다른 조합이 너무 많기 때문에 동일한 벡터 값 집합이 두 번 다시 발생할 가능성은 극히 낮습니다. 우리의 이전 원-핫 표현 및 바이트 쌍 인코딩 전략은 도움이 되지 않습니다.

여기서의 트릭은 이와 같은 밀집된 실수 값 벡터가 단어를 임베딩한 _후_에 얻게 되는 결과라는 점을 주목하는 것입니다. 트랜스포머는 이 형식을 좋아합니다. 이를 활용하기 위해 켑스트럼 전처리 결과를 텍스트 예시의 임베딩된 단어처럼 사용할 수 있습니다. 이렇게 하면 토큰화 및 임베딩 단계를 절약할 수 있습니다.

원하는 다른 모든 유형의 데이터에도 이를 적용할 수 있다는 점에 주목할 가치가 있습니다. 많은 기록된 데이터는 밀집된 벡터 시퀀스 형태로 제공됩니다. 우리는 이들을 임베딩된 단어처럼 트랜스포머의 인코더에 바로 연결할 수 있습니다.

### 마무리 (Wrap up)

이 긴 여정을 함께해 주셔서 감사합니다. 이 글이 유익한 통찰을 제공했기를 바랍니다. 이것으로 우리의 여정은 끝났습니다. 우리는 가상의 음성 제어 컴퓨터를 위한 음성-텍스트 변환기를 만드는 것을 목표로 시작했습니다. 그 과정에서 가장 기본적인 빌딩 블록인 계산과 산술부터 시작하여 트랜스포머를 처음부터 재구성했습니다. 다음번에 최신 자연어 처리 정복에 대한 기사를 읽을 때, 여러분이 만족스럽게 고개를 끄덕이며 내부에서 무슨 일이 일어나고 있는지에 대한 꽤 괜찮은 정신 모델을 가질 수 있기를 바랍니다.

### 자료 및 출처 (Resources and credits)

- ► 원본 [논문](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf "null"), Attention is All You Need.
    
- ► 트랜스포머의 매우 유용한 Python [구현](https://nlp.seas.harvard.edu/2018/04/03/attention.html "null").
    
- ► Jay Alammar의 통찰력 있는 트랜스포머 [설명](https://jalammar.github.io/illustrated-transformer/ "null").
    
- ► Lukasz Kaiser(저자 중 한 명)의 트랜스포머 작동 방식 설명 [강연](https://www.youtube.com/watch?v=rBCqOTEfxvg "null").
    
- ► Google Slides의 [삽화](https://docs.google.com/presentation/d/1Po-GY7X-mXmPKHr8Vh29S4tFPv23TjeY-jq-yShlivM/edit?usp=sharing "null").