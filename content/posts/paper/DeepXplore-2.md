+++
title = 'DeepXplore 2'
date = 2024-07-14T10:05:38+09:00
draft = false
+++

이 글은 DeepXplore: Automated Whitebox Testing of Deep Learning Systems (https://arxiv.org/abs/1705.06640)을 번역한 글입니다.

## 3. Overview

이 섹션에서는 잘못된 코너 케이스 동작에 대해 DNN을 체계적으로 테스트하기 위한 화이트 박스 프레임워크인 DeepXplore에 대한 일반적인 개요를 제공합니다.
DeepXplore의 주요 구성 요소는 그림 5에 나와 있습니다.
DeepXplore는 라벨이 없는 테스트 입력을 시드로 받아 많은 수의 뉴런을 커버하는 새로운 테스트를 생성합니다(즉, 사용자 정의 가능한 임계값 이상의 값으로 활성화됨).
이와 동시에 테스트된 DNN이 다르게 동작하도록 합니다.
구체적으로, DeepXplore는 차별적 동작과 뉴런 커버리지 모두를 극대화하는 공동 최적화 문제를 해결합니다.
이 두 목표는 DNN을 철저히 테스트하고 다양한 잘못된 코너 케이스 동작을 찾는 데 매우 중요합니다.
높은 뉴런 커버리지만으로는 많은 잘못된 동작을 유발하지 못할 수 있으며, 다른 동작만 최대화하면 동일한 근본 원인의 다양한 표현을 식별하는 데 그칠 수 있습니다.

![Figure 5](/posts/paper/DeepXplore/figure5.png)

DeepXplore는 또한 공동 최적화 과정의 일환으로 사용자 정의 도메인별 제약 조건을 적용하는 것을 지원합니다.
예를 들어, 이미지 픽셀의 값은 0과 255 사이여야 합니다.
이러한 도메인별 제약 조건은 DeepXplore 사용자에 의해 지정될 수 있으며, 이를 통해 생성된 테스트 입력이 유효하고 현실적인지 확인할 수 있습니다.

우리는 위에서 언급한 공동 최적화 문제를 효율적으로 해결하기 위해 그래디언트 상승(gradient ascent)을 사용하는 알고리즘을 설계했습니다.
먼저, 입력 값을 변수로 하고 가중치 매개변수를 상수로 하여 출력층과 은닉층의 뉴런 출력에 대한 그래디언트를 계산합니다.
이러한 그래디언트는 대부분의 DNN에서 효율적으로 계산할 수 있습니다.
DeepXplore는 사전 훈련된 DNN에서 작동하도록 설계되었습니다.
그래디언트 계산이 효율적인 이유는 우리 화이트 박스 접근 방식이 사전 훈련된 DNN의 가중치와 중간 뉴런 값에 접근할 수 있기 때문입니다.
다음으로, 우리는 테스트 입력을 반복적으로 수정하여 위에서 설명한 공동 최적화 문제의 목적 함수를 극대화하도록 그래디언트 상승을 수행합니다.
기본적으로, 우리는 시드 입력에서 시작하여 원하는 목표를 극대화하는 새로운 입력을 찾기 위해 그래디언트 유도 지역 탐색을 수행합니다.
주의할 점은, 우리 그래디언트 계산은 DNN 훈련 중에 수행되는 역전파(backpropagation)와 유사하지만, 중요한 차이점은 우리의 알고리즘과 달리 역전파는 입력 값을 상수로, 가중치 매개변수를 변수로 처리한다는 것입니다.

**A working example**
우리는 그림 6을 사용하여 DeepXplore가 테스트 입력을 생성하는 방법을 보여줍니다.
두 개의 DNN이 있다고 가정해 봅시다.
두 DNN은 유사한 작업을 수행하며, 즉 이미지를 자동차나 얼굴로 분류합니다.
그러나 이들은 서로 다른 데이터셋과 매개변수로 독립적으로 훈련되었습니다.
따라서 DNN은 유사하지만 약간 다른 분류 규칙을 학습합니다.
또한, 시드 테스트 입력으로 빨간색 자동차 이미지를 가지고 있다고 가정해 봅시다.
두 DNN 모두 그림 6a와 같이 이 이미지를 자동차로 식별합니다.

![Figure 6](/posts/paper/DeepXplore/figure6.png)

DeepXplore는 입력, 즉 빨간색 자동차 이미지를 수정하여 한 DNN이 이를 자동차로 분류할 확률을 최대화하면서 다른 DNN이 자동차로 분류할 확률을 최소화함으로써 차별적 동작을 찾을 가능성을 극대화하려고 합니다.
DeepXplore는 또한 은닉층의 비활성 뉴런을 활성화(즉, 뉴런 출력이 특정 임계값보다 높아짐)하여 가능한 한 많은 뉴런을 커버하려고 합니다.
우리는 추가로 도메인별 제약 조건을 추가하여 수정된 입력이 여전히 실제 세계의 이미지를 나타내도록 보장합니다(예: 이미지 입력의 픽셀 값이 0과 255 사이의 정수인지 확인).
공동 최적화 알고리즘은 위에서 설명한 모든 목표를 만족하는 수정된 입력을 찾기 위해 반복적으로 그래디언트 상승을 수행합니다.
결국 DeepXplore는 두 DNN의 출력이 다른 테스트 입력 세트를 생성합니다.
예를 들어, 그림 6b와 같이 한 DNN은 이를 자동차로 생각하고 다른 DNN은 이를 얼굴로 생각합니다.

그림 7은 그래디언트 상승을 사용한 우리의 기술의 기본 개념을 설명합니다.
시드 입력에서 시작하여, DeepXplore는 두 개의 유사한 DNN이 동일한 작업을 수행하도록 설계된 입력 공간에서 그래디언트를 따라 유도된 탐색을 수행하여 최종적으로 이러한 DNN의 결정 경계 사이에 있는 테스트 입력을 발견합니다.
이러한 테스트 입력은 두 DNN에 의해 다르게 분류됩니다.
그래디언트가 목표(예: 차이 유발 입력 찾기)를 향한 대략적인 방향을 제공하지만, 가장 빠른 수렴을 보장하지는 않는다는 점을 주의하십시오.
따라서 그림 7에 나와 있듯이, 그래디언트 상승 과정은 종종 목표에 도달하기 위한 직선 경로를 따르지 않습니다.

![Figure 7](/posts/paper/DeepXplore/figure7.png)

## 4. Methodology
이 섹션에서는 우리의 알고리즘에 대한 상세한 기술 설명을 제공합니다.
먼저, DNN을 위한 뉴런 커버리지와 그래디언트의 개념을 정의하고 설명합니다.
다음으로, 테스트 문제를 공동 최적화 문제로 어떻게 공식화할 수 있는지 설명합니다.
마지막으로, 공동 최적화 문제를 해결하기 위한 그래디언트 기반 알고리즘을 제공합니다.

### 4.1 Definitions
**Neuron coverage**
우리는 테스트 입력 세트의 뉴런 커버리지를 모든 테스트 입력에 대해 활성화된 고유한 뉴런의 수와 DNN의 전체 뉴런 수의 비율로 정의합니다.
뉴런의 출력이 임계값(예: 0)보다 높으면 그 뉴런이 활성화된 것으로 간주합니다.

더 형식적으로, 모든 뉴런이 집합 \(N = \{n\relax{1}, n\relax{2}, ...\}\) 으로 표현되고, 모든 테스트 입력이 집합 \(T = \{x\relax{1}, x\relax{2}, ...\}\) 으로 표현된다고 가정합시다.
여기서 \(out(n, x)\)은 주어진 테스트 입력 \(x\)에 대해 DNN의 뉴런 \(n\)의 출력 값을 반환하는 함수입니다.
여기서 굵게 표시된 \(x\)는 \(x\)가 벡터임을 나타냅니다.
뉴런이 활성화되었다고 간주하는 임계값을 \(t\) 로 나타냅시다.
이 설정에서 뉴런 커버리지는 다음과 같이 정의할 수 있습니다.

\[NCov(T, x) = \frac{\left| \{n \mid \forall x \in T, out(n, x) > t\} \right|}{\left| N \right|}\]

실제로 뉴런 커버리지가 어떻게 계산되는지 보여주기 위해, 그림 4b에 표시된 DNN을 고려해 봅시다.
그림 4b에 나와 있는 빨간색 자동차 입력 이미지에 대한 뉴런 커버리지(임계값 0)는 5/8 = 0.625가 될 것입니다.

![Figure 4](/posts/paper/DeepXplore/figure4.png)

**Gradient**
DNN의 뉴런 출력에 대한 입력의 그래디언트 또는 전진 도함수는 딥 러닝 문헌에서 잘 알려져 있습니다.
이들은 적대적 예제를 만들기 위해 [26, 29, 52, 72] 그리고 DNN을 시각화하고 이해하기 위해 [44, 65, 87] 광범위하게 사용되어 왔습니다.
여기서는 완전성을 위해 간단히 정의를 제공하며, 더 자세한 내용을 알고 싶은 독자들은 [87]을 참고하시기 바랍니다.

\(θ\)와 \(x\)가 각각 DNN의 파라미터와 테스트 입력을 나타낸다고 합시다.
뉴런이 수행하는 파라미터 함수는 \(y = f(θ, x)\)로 표현될 수 있으며, 여기서 \(f\)는 \(θ\)와 \(x\)를 입력으로 받아 \(y\)를 출력하는 함수입니다.
\(y\)는 DNN에 정의된 어떤 뉴런의 출력일 수 있습니다(예: 출력층의 뉴런 또는 중간층의 뉴런).
입력 \(x\)에 대한 \(f(θ, x)\)의 그래디언트는 다음과 같이 정의될 수 있습니다:

\[G = \nabla_{x} f(\theta, x) = \frac{\partial y}{\partial x}          \qquad\qquad(1)\]

\(f\) 내부의 계산은 본질적으로 이전 층의 입력을 계산하고 다음 층으로 출력을 전달하는 함수들의 연속입니다.
따라서 \(G\)는 미적분학의 연쇄 법칙을 사용하여 계산할 수 있습니다.
즉, \(y\)를 출력하는 뉴런의 층에서 시작하여 \(x\)를 입력으로 받는 입력 층에 도달할 때까지 층별 도함수를 계산함으로써 얻을 수 있습니다.
그래디언트 \(G\)의 차원은 입력 \(x\)의 차원과 동일하다는 점을 유의하세요.

### 4.2 DeepXplore algorithm
전통적인 소프트웨어와 비교하여 DNN의 테스트 입력 생성 과정의 주요 장점은 최적화 문제로 정의되면 그래디언트 상승을 사용하여 효율적으로 해결할 수 있다는 점입니다.
이 섹션에서는 최적화 문제의 공식화와 해결 방법에 대한 세부 사항을 설명합니다.
최적화 문제의 해답은 DNN의 목적 함수의 그래디언트를 쉽게 계산할 수 있기 때문에, 전통적인 소프트웨어와 달리 DNN에 대해 효율적으로 찾을 수 있다는 점을 유의하십시오.

앞서 §3에서 논의한 바와 같이, 테스트 생성 과정의 목표는 사용자에 의해 제공된 도메인별 제약 조건을 유지하면서 관찰된 차별적 동작의 수와 뉴런 커버리지를 모두 극대화하는 것입니다.
알고리즘 1은 이 공동 최적화 문제를 해결하여 테스트 입력을 생성하는 알고리즘을 보여줍니다.
아래에서는 공동 최적화 문제의 목표를 공식적으로 정의하고 이를 해결하기 위한 알고리즘의 세부 사항을 설명합니다.

**Maximizing differential behaviors**
최적화 문제의 첫 번째 목표는 테스트된 DNN에서 서로 다른 동작을 유도할 수 있는 테스트 입력을 생성하는 것입니다.
즉, 서로 다른 DNN이 동일한 입력을 서로 다른 클래스로 분류하게 하는 것입니다.
n개의 DNN \(F_{k \in 1..n} : \mathbf{x} \rightarrow \mathbf{y}\) 가 있다고 가정합시다.
여기서 \(F_k\)는 k번째 신경망이 모델링한 함수입니다.
\(x\)는 입력을 나타내고 \(y\)는 출력 클래스 확률 벡터를 나타냅니다.
모든 DNN이 동일한 클래스로 분류하는 임의의 시드 \(x\)가 주어졌을 때, 우리의 목표는 수정된 입력 \(x'\)가 적어도 하나의 DNN에 의해 다르게 분류되도록 \(x\)를 수정하는 것입니다.

\(F_k(x)[c]\)가 \(F_k\)가 \(x\)를 \(c\)로 예측하는 클래스 확률이라고 합시다. 우리는 임의로 하나의 신경망 \(F_j\)를 선택하고(알고리즘 1의 6번째 줄) 다음 목적 함수를 최대화합니다:

\[obj_1(\mathbf{x}) = \sum_{k \neq j} F_{k}(\mathbf{x})[c] - \lambda_{1} \cdot F_{j}(\mathbf{x})[c] \qquad\qquad(2)\]

여기에서 \(\lambda_1\)은 DNN \(F_{k \neq j}\)가 이전과 동일한 클래스 출력을 유지하는 목표 항목과 DNN \(F_j\)가 다른 클래스 출력을 생성하는 목표 항목 간의 균형을 맞추기 위한 매개변수입니다.
\(F_{k ∈ 1..n}\)이 모두 미분 가능하므로, 식 2는 계산된 기울기\(\frac{\partial obj_1(\mathbf{x})}{\partial \mathbf{x}}\)를 기반으로 \(x\)를 반복적으로 변경하여 (Algorithm 1의 8-14행과 COMPUTE_OBJ1 절차) 기울기 상승을 사용하여 쉽게 최대화할 수 있습니다.

**Maximizing neuron coverage**
두 번째 목표는 뉴런 커버리지를 극대화하는 입력을 생성하는 것입니다.
우리는 비활성화된 뉴런을 반복적으로 선택하고, 해당 뉴런의 출력이 뉴런 활성화 임계값을 초과하도록 입력을 수정함으로써 이 목표를 달성합니다.
뉴런 \(n\)의 출력을 극대화하려고 한다고 가정해 봅시다.
즉, \(obj_2(x) = f_n(x)\)을 최대화하여 \(f_n(x) > t\)가 되도록 하는 것입니다.
여기서 \(t\)는 뉴런 활성화 임계값이고, \(f_n(x)\)는 \(x\) (DNN의 원래 입력)를 입력으로 받아 뉴런 \(n\)의 출력을 생성하는 함수입니다 (Equation 1에서 정의됨).
\(f_n(x)\)는 그래디언트\(\frac{\partial f_{n}(\mathbf{x})}{\partial \mathbf{x}}\)가 존재하는 미분 가능한 함수이므로, 우리는 다시 그래디언트 상승 메커니즘을 활용할 수 있습니다.

여러 뉴런을 동시에 공동으로 최대화할 수도 있지만, 명확성을 위해 이 알고리즘에서는 한 번에 하나의 뉴런을 활성화하도록 선택합니다(알고리즘 1의 8-14번째 줄 및 COMPUTE_OBJ2 절차).

**Joint optimization**
우리는 위에서 설명한 \(obj_1\)과 \(f_n\)을 공동으로 최대화하며, 다음 함수를 최대화합니다:

\[obj_{joint} = \left(\sum_{i \neq j} F_{i}(\mathbf{x})[c] - \lambda_{1} F_{j}(\mathbf{x})[c] \right) + \lambda_{2} \cdot f_{n}(\mathbf{x}) \qquad\qquad(3)\]

여기서 \(\lambda_2\)는 공동 최적화 과정의 두 목표 사이의 균형을 맞추기 위한 매개변수이며, \(n\)은 각 반복에서 무작위로 선택한 비활성화된 뉴런입니다(알고리즘 1의 33번째 줄).
\(obj_{joint}\)의 모든 항은 미분 가능하므로, \(x\)를 수정하여 그래디언트 상승을 사용해 공동으로 최대화할 수 있습니다(알고리즘 1의 14번째 줄).

**Domain-specific constraints**
최적화 과정에서 중요한 측면 중 하나는 생성된 테스트 입력이 물리적으로 현실적인 여러 도메인별 제약 조건을 만족해야 한다는 것입니다 [63].
특히, 그래디언트 상승 과정의 i번째 반복 동안 \( x_i \)에 적용된 변경 사항이 모든 i에 대해 모든 도메인별 제약 조건을 만족하는지 확인하고자 합니다.
예를 들어, 생성된 테스트 이미지 \( x \)의 경우 픽셀 값이 특정 범위(예: 0에서 255) 내에 있어야 합니다.

일부 제약 조건은 서포트 벡터 머신에서 사용되는 라그랑주 승수법을 사용하여 공동 최적화 과정에 효율적으로 통합될 수 있지만 [76], 대부분의 제약 조건은 최적화 알고리즘에 의해 쉽게 처리될 수 없다는 것을 발견했습니다.
따라서 생성된 테스트가 사용자 정의 도메인별 제약 조건을 충족하도록 보장하는 간단한 규칙 기반 방법을 설계했습니다.
시드 입력 \( x_{seed} = x_0 \)는 정의상 항상 제약 조건을 만족하므로, 우리의 기법은 그래디언트 상승의 i번째 반복 후에도 \( x_i \)가 여전히 제약 조건을 만족하는지 확인해야 합니다(i > 0).
우리의 알고리즘은 그래디언트를 수정하여 이 속성을 보장합니다(알고리즘 1의 13번째 줄).
따라서 \( x_{i+1} = x_i + s \cdot \text{grad} \)가 여전히 제약 조건을 만족하게 합니다(s는 그래디언트 상승의 스텝 크기입니다).

이산형 특징의 경우, 그래디언트를 정수로 반올림합니다.
시각적 입력(예: 이미지)을 처리하는 DNN의 경우, 입력 이미지의 일부분만 수정되도록 다양한 공간 제한을 추가합니다.
우리가 구현한 도메인별 제약 조건에 대한 자세한 설명은 § 6.2에서 확인할 수 있습니다.

**Hyperparameters in Algorithm 1**
요약하자면, DeepXplore의 다양한 측면을 제어하는 네 가지 주요 하이퍼파라미터가 있습니다.
(1) \( \lambda_1 \)는 특정 라벨에 대한 하나의 DNN 예측을 최소화하고 나머지 DNN의 동일 라벨에 대한 예측을 최대화하는 목표 사이의 균형을 맞춥니다.
\( \lambda_1 \)가 클수록 특정 DNN의 예측 값/신뢰도를 낮추는 데 우선순위를 두고, \( \lambda_1 \)가 작을수록 다른 DNN의 예측을 유지하는 데 더 많은 비중을 둡니다.
(2) \( \lambda_2 \)는 차별적 동작 찾기와 뉴런 커버리지 사이의 균형을 제공합니다.
\( \lambda_2 \)가 클수록 다양한 뉴런을 커버하는 데 더 중점을 두고, \( \lambda_2 \)가 작을수록 더 많은 차이 유발 테스트 입력을 생성합니다.
(3) \( s \)는 반복적인 그래디언트 상승 동안 사용되는 스텝 크기를 제어합니다.
\( s \)가 클수록 지역 최적점 주변에서 진동할 수 있으며, \( s \)가 작을수록 목표에 도달하는 데 더 많은 반복이 필요할 수 있습니다.
(4) \( t \)는 각 개별 뉴런이 활성화되었는지 여부를 결정하는 임계값입니다.
\( t \)가 증가할수록 뉴런을 활성화하는 입력을 찾는 것이 점점 더 어려워집니다.

![Algorithm 1](/posts/paper/DeepXplore/algorithm1.png)

## Implementation
우리는 TensorFlow 1.0.1과 Keras 2.0.3 DL 프레임워크를 사용하여 DeepXplore를 구현했습니다.
우리의 구현은 약 7,086줄의 Python 코드로 구성되어 있습니다.
우리의 코드는 TensorFlow/Keras를 기반으로 하지만, 이러한 프레임워크에 대한 수정은 필요하지 않습니다.
우리는 공동 최적화 과정에서 TensorFlow의 효율적인 그래디언트 계산 구현을 활용합니다.
TensorFlow는 또한 임의의 뉴런 출력을 서브-DNN의 출력으로 표시하면서 입력을 원래 DNN의 입력과 동일하게 유지하여 서브-DNN을 생성하는 것을 지원합니다.
우리는 이 기능을 사용하여 DNN의 중간 계층에서 뉴런의 출력을 가로채고 기록하며, DNN의 입력에 대한 해당 그래디언트를 계산합니다.
모든 실험은 Ubuntu 16.04를 실행하는 Linux 노트북(인텔 i7-6700HQ 2.60GHz 프로세서, 4코어, 16GB 메모리, NVIDIA GTX 1070 GPU)에서 실행되었습니다.
