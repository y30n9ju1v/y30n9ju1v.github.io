+++
title = 'ISTQB CT-AI 2장'
date = 2024-10-01T20:45:04+09:00
draft = false
+++

이 글은 ISTQB의 CT-AI(Certified Tester AI Testing) 자격증을 준비하면서 한국어 실라버스를 참고하여 정리 요약한 글입니다.
실라버스는 아래 링크에서 다운 받을 수 있습니다.

https://www.kstqb.org/board_skin/board_view.asp?idx=718&page=1&bbs_code=4&key=0&word=&etc=

## 2. 인공지능기반 시스템 품질 특성
### 2.1 유연성과 적응성
유연성은 시스템이 본래의 시스템 요구사항의 일부가 아닌 상황에서 얼마만큼 사용될 수 있는가 하는 정도이고 적응성은 의도와는 다른 하드웨어나 변화하는 운영 호나경 등의 새로운 상황에 맞게 제품을 수정하려고 할 때의 용이성을 말한다.

유연성과 적응성 모두 다음과 같은 상황에 유용하다.
* 시스템 배포 시 운영 환경을 완전히 알 수 없는 경우
* 시스템이 새로운 운영 환경에 대처할거라고 기대하는 경우
* 시스템이 새로운 상황에 적응할거라고 기대하는 경우
* 시스템이 언제 동작을 수정해야 하는지 결정해야 하는 경우

인공지능기반 시스템의 우연성과 적응성 요구사항에는 시스템이 적응해야 할 환경 변화에 대한 세부 사항이 포함되어야 한다.

### 2.2 자율성
자율성이란 시스템이 사람의 감독과 통제로부터 독립적으로 일정 기간동안 동작할 수 있는 능력을 말한다. 
인공지능기반 시스템 중 일부는 자율 시스템에 해당하지만 모든 인공지능기반 시스템이 자율 시스템인것은 아니다. 예를 들어 일반적으로 "자율주행"이라고 불리는 완전 자율주행 자동차는 공식적으로 "완전 주행 자동화"로 분류된다. 

### 2.3 진화
진화는 변화하는 외부 제약에 대응해 스스로 개선하는 시스템의 능력을 말하며 일부 인공지능 시스템은 자가학습 시스템으로 분류되며 성공적인 자가학습 인공지능기반 시스템은 이런 진화가 필요하다.

자가학습 인공지능기반 시스템은 일반적으로 두 가지 형태의 변화를 관리해야 한다.
* 변화의 한 형태는 시스템이 자신의 결정 및 주변환경과의 상호작용에서 학습하는 경우
* 변화의 또 다른 형태는 시스템이 시스템 운영 환경에 가해진 변경 사항에서 학습하는 경우

두 경우 모두 시스템이 자신의 효과성과 효율성을 향상시키기 위해 진화하는 것이 이상적이다. 그러나 이런 진화과정에서 원하지 않는 시스템 특성이 나타나지 않도록 통제가 필요하다. 모든 진화는 원래의 시스템 요구 사항과 제약 조건을 지속적으로 충족해야 한다.

### 2.4 편향성
인공지능기반 시스템에서 편향성이란 시스템이 제공하는 결과와 "공정한 결과"로 간주되는 값 간의 거리차이를 나타내는 통계적 수치이다. 공정한 결과는 특ㅈ넝 그룹에 대한 편애를 나타내지 않는 결과를 뜻한다. 부적절한 편경으로 설병, 인종, 민족, 설정 지향, 소득 수준, 나이와 같은 속성을 들수 있다. 인공지능기반 시스템의 부적절한 편향 사례는 은행 대출심사 시스템, 채용 시스템, 사법 모니터링 시스템 등에서 보고되었다.

기계학습 시스템은 수집된 데이터를 사용하는 알고리즘을 가지고 의사 결정과 예측을 하는데 사용되며 이 두가지 컴포넌트는 결과에 편향성을 일으킬 수 있다.
* 학습 알고리즘이 잘못 구성된 경우
* 훈련 데이터가 기계학습이 적용되는 데이터 공간을 충분히 나타내지 못하는 경우

### 2.5 윤리
고도화된 기능을 갖춘 인공지능기반 시스템은 사람들의 삶에 대체로 긍정적인 영향을 미치고 있다. 이런 시스템이 점점 보편화되면서 그것이 윤리적으로 사용되는지에 대한 우려도 제기되고 있다. 

경제협력개발기구(OECD)는 2019년에 인공지능의 책임있는 개발을 위해 최초로 정부간 합의된 국제 표준인 인공지능 권고안을 발표했다. 이 권고안은 발표 당시 42개국에서 채택되었으며 유렵연합 집행위원회의 지지를 받았다. 여기에는 "신뢰할 수 있는 인공지능의 책임있는 관리"를 위한 실질적인 정책권고와 가치 기반 원칙이 담겨 있으며 다음과 같이 요약된다.
* 인공지능은 포괄적 성장, 지속가능한 개발, 웰빙을 이끌어 인간과 지구에 도움이 돼야 한다.
* 인공지능 시스템은 법치, 인권, 민주적 가치, 다양성의 가치를 존중하고 공정한 사회 보장을 위해 필요한 적절한 보호 장치를 포함해야 한다.
* 결과를 사람이 이해할 수 있고 이의를 제기할 수 있도록 인공지능 전반에 대한 투명성이 있어야 한다.
* 인공지능 시스템은 수명이 다할 때까지 견고하고 보안이 확보된 안전한 방식으로 가능해야 하며 잠재적 위험은 지속적으로 평가돼야 한다.
* 인공지능 시슽메을 개발, 배포, 운영하는 조직 및 개인은 이에 대해 책임을 져야 한다.

### 2.6 부작용과 보상 해킹
부작용은 인공지능기반 시스템의 설계자가 다음과 같은 목표를 설정했을 때 생길 수 있다.
"주어진 환경에서 특정 작업을 수행하는 데만 중점을 두고 (잠재적으로 매우 클 수 있는) 환경의 다른 측면을 고려하지 않아 실제로 변화에 해로운 환경 변수에 대한 무관심을 은영중에 표현". 가령 "최대한 연료를 절약하면서 안전한 방법"으로 목적지까지 이동하는 것을 목표로 하는 자율주행 잗동차가 목표는 달성하지만 이동 시간이 너무 오래 걸려 탑승자들이 극도로 짜증을 내는 부작용을 초래하는 것을 예로 들수 있다.

보상 해킹은 인공지능기반 시스템이 "설계자의 의도를 왜곡하는" "영리한" 또는 "쉬운" 해결책을 사용해 정해진 목표를 달성함으로써 결국 목표가 왜곡되는 결과를 가져오는 것이다. 보상해킹에 대한 예시로 널리 사용되는 사례는 인공지능기반 시스템이 아케이드 컴퓨터 게임을 스스로 학습하는데 '최고 점수' 달성을 목표로 정했을 때 이를 위해 게임을 하지 않고 저장된 최고 점수 데이터 기록을 해킹하는 경우를 들 수 있다.

### 2.7 투명성, 해석 가능성, 설명 가능성
인곤지능기반 시스템은 일반적으로 사용자가 시스템을 신뢰해야 하는 영역에 적용된다. 안전상의 이유일 수도 있지만 사생활 보호가 필요하거나 잠재적으로 삶을 변화시키는 예측 및 결정을 제공해야 하는 경우일 수도 있다. 대부분의 사용자는 "블랙박스"로 인공지능기반 시스템을 접하게 되며 해당 시스템이 결과에 어떻게 도달했는지는 거의 알지 못한다.

인공지능기반 시스템이 가지는 복잡성은 "설명 가능한 인공지능"의 출현으로 이어졌다. ㅌ먀의 목표는 사용자가 인공지능기반 시스템이 어떻게 결과를 도출하는지 이해할 수 있도록 함으로써 사용자 신뢰를 높이는 것이다.

런던 왕립학회에 따르면 XAI가 필요한 이유는 다음을 포함해 여러 가지가 있다.
* 사용자에게 시스템에 대한 신뢰 제공
* 편향성으로부터 보호
* 규제 표준 또는 정책 요구사항 충족
* 시스템 설계 개선
* 리스크, 강건성, 취약성 평가
* 시스템 출력의 이해 및 확인
* 자율성, 에이전시, 사회적 가치 충족

이해관계자의 관점에서 인공지능기반 시스템에 대해 다음과 같은 세 가지 이상적인 XAI 기본 특성으로 이어진다.
* 투명성: 모델을 생성하는데 사용된 알고리즘과 훈련 데이터를 얼마나 쉽게 식별할 수 있는지
* 해석 가능성: 사용자와 다양한 이해관계자가 사용된 인공지능 기술을 얼마나 이해할 수 있는지
* 설명 가능성: 사용자가 인공지능기반 시스템이 특정 결과를 어떻게 도출했는지를 얼마나 쉽게 판단할 수 있는지

### 2.8 안전과 인공지능
안전이란 인공지능기반 시스템이 사람, 재산 또는 환경에 해를 끼치지 않을 것이라는 기대를 말한다. 안전에 영향을 미치는 의사 결정에 인공지능기반 시스템이 사용될 수 있다. 예를 들어 의료, 제조, 국방, 보안, 운송 분야에서 사용하는 인공지능기반 시스템은 안전에 영향을 미칠 가능성이 있다.

인공지능기반 시스템이 안전한지 검증하는 것을 더 어렵게 만드는 특성으로 다음과 같은 것들이 있다.
* 복잡성
* 비결정성
* 태생적 확률성
* 자가학습
* 투명성, 해석 가능성, 설명 가능성 부족
* 강건성 부족

